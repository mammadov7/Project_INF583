{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lower, col, size, length\n",
    "from operator import add\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession\\\n",
    "  .builder \\\n",
    "  .appName(\"Twitter_app\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre',\n",
       " 'http',\n",
       " 'https']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading from the stopword file:\n",
    "text_file = open(\"datasets/stop_words_english.txt\", \"r\")\n",
    "lines = text_file.read()\n",
    "\n",
    "## Creating the stop word array:\n",
    "stopWords = lines.split()\n",
    "# Adding http and https to it:\n",
    "stopWords.append('http')\n",
    "stopWords.append('https')\n",
    "stopWords[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = spark.read.format('json').options(header='true', inferSchema='true') \\\n",
    "  .load('./datasets/NoFilterEnglish2020-02-04.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|text                                                     |\n",
      "+---------------------------------------------------------+\n",
      "|@theythemsbian thank you for being brave enough to say it|\n",
      "+---------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "clean_texts = data.select('*', lower(f.regexp_replace(f.col('text'), r'[^a-zA-Z#@,!\\\\s]', ' ')).alias('text2'))\\\n",
    "                  .drop('text')\\\n",
    "                  .withColumnRenamed('text2', 'text')\n",
    "\n",
    "clean_texts.select('text').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|words_t                                                             |\n",
      "+--------------------------------------------------------------------+\n",
      "|[@theythemsbian, thank, you, for, being, brave, enough, to, say, it]|\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words_t')\n",
    "clean_word_tokens = tokenizer.transform(clean_texts)\n",
    "\n",
    "clean_word_tokens.select('words_t').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove useless words:\n",
    "\n",
    "By useless, we mean the words of size 2 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|words                                                       |\n",
      "+------------------------------------------------------------+\n",
      "|[@theythemsbian, thank, you, for, being, brave, enough, say]|\n",
      "+------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Defining the function: (udf: user defined function)\n",
    "filter_length_udf = f.udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "clean_word_tokens = clean_word_tokens.withColumn('words', filter_length_udf(col('words_t')))\n",
    "\n",
    "clean_word_tokens.select('words').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                text|          word|\n",
      "+--------------------+--------------+\n",
      "|@theythemsbian th...|@theythemsbian|\n",
      "|@theythemsbian th...|         thank|\n",
      "|@theythemsbian th...|           you|\n",
      "|@theythemsbian th...|           for|\n",
      "+--------------------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words = clean_word_tokens.withColumn('word', f.explode('words'))\\\n",
    "                             .drop('words')\n",
    "                 \n",
    "#Tokenizer\n",
    "exp_words.select('text', 'word').show(4)\n",
    "\n",
    "exp_words.createOrReplaceTempView(\"words\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreqWords(ts1, ts2, table = \"words\"):\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT word, count(*) AS word_count \" + \n",
    "                      \"FROM {} \".format(table) + \n",
    "                      \"WHERE {0}.timestamp_ms BETWEEN {1} AND {2} \".format(table, ts1, ts2)+ \n",
    "                      \"GROUP BY word \" + \n",
    "                      \"ORDER BY word_count DESC\")\n",
    "    sqlDF.show(10)\n",
    "    return sqlDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| word|word_count|\n",
      "+-----+----------+\n",
      "|  the|        13|\n",
      "|https|         7|\n",
      "|  you|         6|\n",
      "| this|         6|\n",
      "|  for|         4|\n",
      "|  can|         4|\n",
      "| make|         3|\n",
      "| need|         3|\n",
      "|  not|         2|\n",
      "| rare|         2|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.339\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   13|\n",
      "|https|    7|\n",
      "|  you|    6|\n",
      "| this|    6|\n",
      "|  for|    4|\n",
      "|  can|    4|\n",
      "| make|    3|\n",
      "| need|    3|\n",
      "| rare|    2|\n",
      "|  not|    2|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.297\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "## Timing the operation:\n",
    "t1 = time.time()\n",
    "\n",
    "exp_words.filter(f.col('timestamp_ms').between(ts1, ts2) )\\\n",
    "         .groupBy('word') \\\n",
    "         .count() \\\n",
    "         .sort('count', ascending=False) \\\n",
    "         .show(10)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "#         .filter(length(col(\"word\")) >= 3)\\\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "\n",
    "We can see that both methods are equivalent in computing time and yield to the same results. Choosing between both is just a question of taste. We Personally prefer **SQL** querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|filtered                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|[@theythemsbian, brave]                                                                                    |\n",
      "|[@camillediola, duterte, violated, law, appointing, honasan,, background,, head, dict, shrugged,, pointing]|\n",
      "|[@tinyseokjinnie, lol, bored, dypgt]                                                                       |\n",
      "|[@ibesuckafree, suck, quitting, weed, argument, amp, gettin]                                               |\n",
      "|[@mikebloomberg, donald, trump, bring, change, country, #superbowl, vciycilow]                             |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing the stopwords from the array of strings\n",
    "sc = spark.sparkContext\n",
    "broadcastVar = sc.broadcast(stopWords)\n",
    "broadcastVar.value\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=broadcastVar.value)\n",
    "words_f = remover.transform(clean_word_tokens)\n",
    "words_f = words_f.drop('words_t', 'words')\n",
    "\n",
    "words_f.select('filtered').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['truncated', 'user', 'withheld_in_countries', 'text', 'filtered']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words_filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+--------------+\n",
      "|text                                                     |word          |\n",
      "+---------------------------------------------------------+--------------+\n",
      "|@theythemsbian thank you for being brave enough to say it|@theythemsbian|\n",
      "+---------------------------------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words_f = words_f.withColumn('word', f.explode('filtered'))#\\\n",
    "                     #.drop('filtered')\n",
    "#.drop('filtered')???\n",
    "\n",
    "exp_words_f.select('text', 'word').show(1, False)\n",
    "\n",
    "exp_words_f.createOrReplaceTempView(\"words_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'withheld_in_countries', 'text', 'filtered', 'word']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time after filtering using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|         word|word_count|\n",
      "+-------------+----------+\n",
      "|          pro|         2|\n",
      "|         rare|         2|\n",
      "|         rush|         2|\n",
      "|          amp|         2|\n",
      "|         fuck|         2|\n",
      "|    @yungcalc|         1|\n",
      "|    @santeira|         1|\n",
      "|        paral|         1|\n",
      "|@chqmbiedolan|         1|\n",
      "|     whaaaaat|         1|\n",
      "+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.306\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words_filtered\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|            word|count|\n",
      "+----------------+-----+\n",
      "|             don| 7919|\n",
      "|             amp| 7470|\n",
      "|          people| 6621|\n",
      "|            love| 6304|\n",
      "|            time| 5231|\n",
      "|             day| 4775|\n",
      "|           trump| 4621|\n",
      "|            good| 4434|\n",
      "|           today| 3708|\n",
      "|            iowa| 3488|\n",
      "|            shit| 3213|\n",
      "|             man| 3156|\n",
      "|            life| 2527|\n",
      "|           happy| 2424|\n",
      "|            fuck| 2337|\n",
      "|            year| 2273|\n",
      "|           years| 2206|\n",
      "|           black| 2127|\n",
      "|@realdonaldtrump| 2107|\n",
      "|           great| 2065|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words_f.filter(\"(timestamp_ms / 1000 / 60 / 60  % 24 )>= 20\") \\\n",
    "           .groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = exp_words_f.filter( exp_words_f.word.startswith('#'))\\\n",
    "                      .withColumnRenamed('word', 'hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             hashtag|count|\n",
      "+--------------------+-----+\n",
      "|         #curecancer| 3987|\n",
      "|       #iowacaucuses| 3253|\n",
      "|#deshkiawaazshehnaaz| 2871|\n",
      "|    #mainbhiasimkigf| 2782|\n",
      "|      #meraadarshsid| 2685|\n",
      "|       #iheartawards| 1753|\n",
      "|                #bts| 1658|\n",
      "|           #asimriaz| 1512|\n",
      "|        #thebachelor| 1358|\n",
      "|         #iowacaucus| 1274|\n",
      "|                 #bb| 1251|\n",
      "|     #sidharthshukla| 1245|\n",
      "|           #treasure| 1232|\n",
      "|        #coronavirus| 1197|\n",
      "|           #iacaucus| 1197|\n",
      "|           #biggboss| 1156|\n",
      "|         #loveisland|  999|\n",
      "|          #superbowl|  976|\n",
      "|     #worldcancerday|  965|\n",
      "|               #sotu|  894|\n",
      "|     #rightchoicesid|  843|\n",
      "|              #yikes|  776|\n",
      "|     #happyjisungday|  710|\n",
      "|               #iowa|  696|\n",
      "|            #btsarmy|  686|\n",
      "|         #nowplaying|  646|\n",
      "|                #nct|  640|\n",
      "|       #rashamidesai|  582|\n",
      "|                #raw|  581|\n",
      "|             #superm|  574|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_freq_hash = hashtags.groupBy('hashtag') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False) \n",
    "most_freq_hash.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting Hashtag Data Frame to array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfht = most_freq_hash.filter('count >= 500').select('hashtag').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting words related to Hashtag example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#iowacaucuses',\n",
       " 'iowa',\n",
       " 'bernie',\n",
       " 'caucus',\n",
       " 'vote',\n",
       " 'won',\n",
       " 'tonight',\n",
       " '#iowacaucus',\n",
       " 'biden',\n",
       " 'app',\n",
       " '@donaldjtrumpjr',\n",
       " 'dnc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_words = exp_words_f.filter( f.col('text').contains('#iowacaucuses'))\n",
    "\n",
    "event_words.groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False)\\\n",
    "           .rdd\\\n",
    "           .map(lambda x: x[0])\\\n",
    "           .collect()[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex5 using pySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+\n",
      "|hashtag|       word|count|\n",
      "+-------+-----------+-----+\n",
      "| #yikes|@nickiminaj|  465|\n",
      "| #yikes|      likes|  348|\n",
      "| #yikes|        amp|  336|\n",
      "| #yikes|       play|  325|\n",
      "| #yikes|       life|  313|\n",
      "| #yikes|        tag|  313|\n",
      "| #yikes|    yzdifvf|  310|\n",
      "| #yikes|       rosa|  137|\n",
      "| #yikes|      parks|  129|\n",
      "| #yikes|      nicki|  121|\n",
      "+-------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ex.5: Get the most frequent words for each event:\n",
    "ht_f = hashtags.select('filtered', 'hashtag')\\\n",
    "                         .filter(f.col('hashtag').isin(mfht))\\\n",
    "                         .withColumn('word', f.explode('filtered'))\\\n",
    "                         .filter(~col('word').startswith('#'))\\\n",
    "                         .select('hashtag', 'word')\\\n",
    "                         .groupBy('hashtag', 'word')\\\n",
    "                         .count()\\\n",
    "                         .sort(['hashtag', 'count'], ascending=False)\\\n",
    "                         .filter('count >= 100')\n",
    "\n",
    "ht_f.show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the top 10 most frequent words by events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|    hashtag|       word|count|\n",
      "+-----------+-----------+-----+\n",
      "|#curecancer|trueworship| 3951|\n",
      "|#curecancer|     cancer| 2441|\n",
      "|#curecancer|       true| 1240|\n",
      "|#curecancer|     rampal| 1073|\n",
      "|#curecancer|    worship| 1042|\n",
      "|#curecancer|      saint| 1040|\n",
      "|#curecancer|       cure|  916|\n",
      "|#curecancer|        god|  884|\n",
      "|#curecancer|    maharaj|  859|\n",
      "|#curecancer|   diseases|  846|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+---------------+-----+\n",
      "|      hashtag|           word|count|\n",
      "+-------------+---------------+-----+\n",
      "|#iowacaucuses|           iowa|  499|\n",
      "|#iowacaucuses|         bernie|  346|\n",
      "|#iowacaucuses|         caucus|  237|\n",
      "|#iowacaucuses|           vote|  193|\n",
      "|#iowacaucuses|            won|  173|\n",
      "|#iowacaucuses|        tonight|  155|\n",
      "|#iowacaucuses|          biden|  153|\n",
      "|#iowacaucuses|            app|  151|\n",
      "|#iowacaucuses|@donaldjtrumpjr|  150|\n",
      "|#iowacaucuses|            dnc|  147|\n",
      "+-------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+-----+\n",
      "|             hashtag|     word|count|\n",
      "+--------------------+---------+-----+\n",
      "|#deshkiawaazshehnaaz|     sana|  459|\n",
      "|#deshkiawaazshehnaaz|     love|  287|\n",
      "|#deshkiawaazshehnaaz|     guys|  235|\n",
      "|#deshkiawaazshehnaaz| shehnaaz|  193|\n",
      "|#deshkiawaazshehnaaz|    trend|  143|\n",
      "|#deshkiawaazshehnaaz|@colorstv|  141|\n",
      "|#deshkiawaazshehnaaz|    speed|  128|\n",
      "|#deshkiawaazshehnaaz| trending|  119|\n",
      "|#deshkiawaazshehnaaz|   follow|  116|\n",
      "|#deshkiawaazshehnaaz|      hai|  113|\n",
      "+--------------------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-----------+-----+\n",
      "|         hashtag|       word|count|\n",
      "+----------------+-----------+-----+\n",
      "|#mainbhiasimkigf|       asim|  982|\n",
      "|#mainbhiasimkigf|      trend|  485|\n",
      "|#mainbhiasimkigf|      girls|  482|\n",
      "|#mainbhiasimkigf|   trending|  291|\n",
      "|#mainbhiasimkigf|  @colorstv|  253|\n",
      "|#mainbhiasimkigf|       love|  214|\n",
      "|#mainbhiasimkigf|   fangirls|  202|\n",
      "|#mainbhiasimkigf|        fan|  193|\n",
      "|#mainbhiasimkigf|  @biggboss|  187|\n",
      "|#mainbhiasimkigf|@imrealasim|  186|\n",
      "+----------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+---------------+-----+\n",
      "|       hashtag|           word|count|\n",
      "+--------------+---------------+-----+\n",
      "|#meraadarshsid|            sid|  516|\n",
      "|#meraadarshsid|         shukla|  428|\n",
      "|#meraadarshsid|      @sidharth|  352|\n",
      "|#meraadarshsid|         tweets|  282|\n",
      "|#meraadarshsid|@realvindusingh|  281|\n",
      "|#meraadarshsid|          today|  228|\n",
      "|#meraadarshsid|            tag|  223|\n",
      "|#meraadarshsid|      @biggboss|  180|\n",
      "|#meraadarshsid|       activity|  162|\n",
      "|#meraadarshsid|        tagline|  160|\n",
      "+--------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+----------------+-----+\n",
      "|      hashtag|            word|count|\n",
      "+-------------+----------------+-----+\n",
      "|#iheartawards|            @bts|  567|\n",
      "|#iheartawards|             twt|  553|\n",
      "|#iheartawards|            vote|  536|\n",
      "|#iheartawards|             sos|  264|\n",
      "|#iheartawards|   @justinbieber|  259|\n",
      "|#iheartawards|     @danandshay|  226|\n",
      "|#iheartawards|            @tha|  220|\n",
      "|#iheartawards|@ygofficialblink|  216|\n",
      "|#iheartawards|          voting|  200|\n",
      "|#iheartawards|           hours|  187|\n",
      "+-------------+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    ht_f.filter(col('hashtag').isin(ht)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that converts geolocalisation to place name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(coords):\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")  \n",
    "    if coords.shape != (2,):\n",
    "        coords = coords.mean(axis = 0)\n",
    "    \n",
    "    Longitude = str(coords[0])\n",
    "    Latitude = str(coords[1])\n",
    "    \n",
    "    location = geolocator.reverse(Latitude+\",\"+Longitude)\n",
    "    \n",
    "    if(location == None):\n",
    "        print(\"No location specified\")\n",
    "        return\n",
    " \n",
    "    address = location.raw['address']\n",
    "\n",
    "    # traverse the data\n",
    "    city = address.get('city', '')\n",
    "    state = address.get('state', '')\n",
    "    country = address.get('country', '')\n",
    "    code = address.get('country_code')\n",
    "    zipcode = address.get('postcode')\n",
    "    print('City : ', city)\n",
    "    print('State : ', state)\n",
    "    print('Country : ', country)\n",
    "    print('Zip Code : ', zipcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 and Exercice 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City :  \n",
      "State :  Madhya Pradesh\n",
      "Country :  India\n",
      "Zip Code :  476332\n",
      "Event : #curecancer\n",
      "Timeframe : from - 2020-02-04 03:16:13 to - 2020-02-04 22:42:39\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#curecancer', 'trueworship', 'cancer', 'true', 'rampal', 'saint', 'worship', 'cure', 'god', 'maharaj', 'diseases', 'disease']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  West Virginia\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #iowacaucuses\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:58:28\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucuses', 'iowa', 'bernie', 'caucus', 'vote', 'won', 'tonight', '#iowacaucus', 'biden', 'app', '@donaldjtrumpjr', 'dnc']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Maharashtra\n",
      "Country :  India\n",
      "Zip Code :  431201\n",
      "Event : #deshkiawaazshehnaaz\n",
      "Timeframe : from - 2020-02-04 01:58:49 to - 2020-02-04 22:59:12\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#deshkiawaazshehnaaz', 'sana', 'love', 'guys', 'shehnaaz', 'trend', '@colorstv', 'speed', 'trending', 'follow', 'hai', 'tag']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #mainbhiasimkigf\n",
      "Timeframe : from - 2020-02-03 23:26:18 to - 2020-02-04 22:58:06\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#mainbhiasimkigf', 'asim', 'trend', 'girls', 'trending', '#asimriaz', '@colorstv', 'love', 'fangirls', 'fan', '@imrealasim', '@biggboss']\n",
      "\n",
      "\n",
      "City :  Mumbai\n",
      "State :  Maharashtra\n",
      "Country :  India\n",
      "Zip Code :  400065\n",
      "Event : #meraadarshsid\n",
      "Timeframe : from - 2020-02-04 08:30:32 to - 2020-02-04 22:59:16\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#meraadarshsid', 'sid', 'shukla', '@sidharth', '@realvindusingh', 'tweets', 'today', 'tag', '@biggboss', 'activity', 'tagline', '@herdhush']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #iheartawards\n",
      "Timeframe : from - 2020-02-03 23:00:38 to - 2020-02-04 22:58:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iheartawards', '@bts', 'twt', 'vote', '#bestfanarmy', '#bestmusicvideo', '#bestlyrics', '#bestcoversong', '#btsarmy', 'sos', '@justinbieber', '#boywithluv']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  North Carolina\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #bts\n",
      "Timeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:59:54\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#bts', '@bts', 'twt', '#btsarmy', 'bts', 'soul', 'outro', 'comeback', 'trailer', '#bestfanarmy', '#btsxzachsang', '@bighitent']\n",
      "\n",
      "\n",
      "City :  Mumbai\n",
      "State :  Maharashtra\n",
      "Country :  India\n",
      "Zip Code :  400065\n",
      "Event : #asimriaz\n",
      "Timeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:59:07\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#asimriaz', 'asim', '#mainbhiasimkigf', 'fans', 'amp', '#sidharthshukla', '#biggboss', '#bb', '#asimfandomhits', '#rashamidesai', 'love', '@biggboss']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kentucky\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #thebachelor\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:54:09\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#thebachelor', 'peter', 'kelsey', 'kelley', 'tammy', 'mykenna', '#thebachelorabc', 'drama', 'bachelor', 'girls', 'season', 'rose']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Ohio\n",
      "Country :  United States\n",
      "Zip Code :  45662\n",
      "Event : #iowacaucus\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucuses', '#iowacaucus', 'iowa', '#iowacaucusdisaster', 'bernie', 'caucus', 'app', 'breaking', 'party', 'vote', 'democrats', 'dnc']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kogi\n",
      "Country :  Nigeria\n",
      "Zip Code :  None\n",
      "Event : #bb\n",
      "Timeframe : from - 2020-02-03 23:05:20 to - 2020-02-04 22:58:37\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#bb', '#biggboss', '#bbb', 'shukla', '#sidharthshukla', 'prior', 'pyong', '@cleytu', 'tweet', 'appreciation', 'thnbpfwynp', 'jantando']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #sidharthshukla\n",
      "Timeframe : from - 2020-02-03 23:15:22 to - 2020-02-04 22:56:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#sidharthshukla', 'amp', '#bb', 'shukla', '#biggboss', '@sidharth', '#asimriaz', 'sid', '#meraadarshsid', 'hai', '#rightchoicesid', '#shehnaazgill']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #treasure\n",
      "Timeframe : from - 2020-02-03 23:01:44 to - 2020-02-04 22:47:25\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#treasure', 'editorial', 'treasure', 'week', 'maker', 'photography', 'official', '@ygent', '#vol', 'bang', 'dam', 'asahi']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Missouri\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #iacaucus\n",
      "Timeframe : from - 2020-02-03 23:04:16 to - 2020-02-04 22:59:04\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iacaucus', 'iowa', '#iowacaucuses', '@andrewgillum', '@gopchairwoman', 'wait', 'precincts', 'delays', 'hear', 'bad', 'winner', 'black']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #coronavirus\n",
      "Timeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:59:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#coronavirus', 'china', 'evil', '#coronavirusoutbreak', '#china', 'cases', 'chinese', '#coronaviruschina', 'confirmed', 'medical', 'hospital', 'patients']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Telangana\n",
      "Country :  India\n",
      "Zip Code :  500089\n",
      "Event : #biggboss\n",
      "Timeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:56:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#biggboss', '#bb', '#sidharthshukla', 'asim', '@colorstv', 'amp', 'fans', 'riaz', 'boss', '#asimriaz', 'bigg', 'shukla']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #loveisland\n",
      "Timeframe : from - 2020-02-03 23:00:30 to - 2020-02-04 22:59:25\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#loveisland', 'callum', 'shaughna', 'casa', 'mike', 'amor', 'ched', 'villa', 'girl', 'luke', 'boy', 'molly']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #superbowl\n",
      "Timeframe : from - 2020-02-03 22:59:55 to - 2020-02-04 22:52:53\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#superbowl', 'trump', '#superbowlliv', 'anthem', 'national', 'halftime', 'mocking', '@shakira', 'stand', 'caught', 'gop', 'takes']\n",
      "\n",
      "\n",
      "City :  Sidi Ghanem سيدي غانم\n",
      "State :  \n",
      "Country :  Maroc / ⵍⵎⵖⵔⵉⴱ / المغرب\n",
      "Zip Code :  None\n",
      "Event : #worldcancerday\n",
      "Timeframe : from - 2020-02-03 23:29:10 to - 2020-02-04 22:58:40\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#worldcancerday', 'cancer', 'today', 'disease', 'day', 'amp', 'people', '#cancer', 'awareness', 'support', 'terrible', 'battling']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #sotu\n",
      "Timeframe : from - 2020-02-03 23:08:18 to - 2020-02-04 22:59:43\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#sotu', 'tomorrow', 'trump', 'president', 'tonight', 'attend', 'join', 'people', '@potus', '@flotus', 'capitol', 'inspiring']\n",
      "\n",
      "\n",
      "City :  Karnal\n",
      "State :  Haryana\n",
      "Country :  India\n",
      "Zip Code :  132001\n",
      "Event : #rightchoicesid\n",
      "Timeframe : from - 2020-02-03 23:00:39 to - 2020-02-04 21:47:09\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#rightchoicesid', 'sid', 'shukla', '@sidharth', '@realvindusingh', '#sidharthshukla', 'guys', '#bb', 'trend', 'win', '@biggboss', '@colorstv']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kansas\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #yikes\n",
      "Timeframe : from - 2020-02-04 03:15:25 to - 2020-02-04 22:57:35\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#yikes', '@nickiminaj', '#yikesnm', 'likes', 'amp', 'play', 'life', '#it', 'tag', 'yzdifvf', 'nicki', 'rosa']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #happyjisungday\n",
      "Timeframe : from - 2020-02-04 03:27:07 to - 2020-02-04 22:59:53\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#happyjisungday', 'birthday', 'happy', '@nctsmtown', '#nct', '#nctdream', '#jisung', 'slthg', 'jisung', 'park', 'baby', 'dream']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kentucky\n",
      "Country :  United States\n",
      "Zip Code :  41006\n",
      "Event : #iowa\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucuses', '#iowacaucus', 'iowa', '#iowa', '#iowacaucusdisaster', 'bernie', 'caucus', '#iowacaucas', 'app', 'democrats', 'tonight', 'party']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Wisconsin\n",
      "Country :  United States\n",
      "Zip Code :  54241\n",
      "Event : #btsarmy\n",
      "Timeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:58:56\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#btsarmy', '@bts', 'twt', '#bestfanarmy', '#iheartawards', 'message', 'bts', '@choi', 'album', 'era', '@btsvotingteam', 'army']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #nowplaying\n",
      "Timeframe : from - 2020-02-03 23:00:17 to - 2020-02-04 22:56:54\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#nowplaying', 'listen', 'radio', 'live', 'amp', 'tune', 'love', 'feat', '#listenlive', 'music', 'mix', '#radio']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #nct\n",
      "Timeframe : from - 2020-02-03 23:09:25 to - 2020-02-04 22:59:53\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#nct', '#nctdream', '#happyjisungday', '#jisung', 'birthday', 'happy', '@nctsmtown', 'slthg', '#superm', 'dream', '#taeyong', 'icn']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #rashamidesai\n",
      "Timeframe : from - 2020-02-03 23:03:45 to - 2020-02-04 22:58:18\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#rashamidesai', '#asimriaz', 'amp', '#sidharthshukla', '#invinciblerashamidesai', 'khabri', '@biggboss', '@real', '#bb', 'breaking', '@therashamidesai', 'elite']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Missouri\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #raw\n",
      "Timeframe : from - 2020-02-03 23:07:33 to - 2020-02-04 22:56:17\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#raw', '@wwe', 'wwe', '@wweuniverse', '#wwe', '@wweasuka', '@rubyriottwwe', '@randyorton', 'tonight', 'rematch', '@angelgarzawwe', 'challenge']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  North Carolina\n",
      "Country :  United States\n",
      "Zip Code :  27239:27292\n",
      "Event : #superm\n",
      "Timeframe : from - 2020-02-03 23:01:58 to - 2020-02-04 22:58:32\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#superm', 'live', '@superm', 'superm', 'san', 'jose', 'future', '#superminsanjose', '#superminsj', '#wearethefuture', '#supermthefuture', '#ten']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #sb\n",
      "Timeframe : from - 2020-02-03 23:01:37 to - 2020-02-04 22:58:54\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#sb', '@sb', 'official', 'billboardmainstay', '#wishbustuessb', '#sbliv', '#morhot', '@mor', 'alab', 'onawesamsungcon', '@nfl', '@jlo']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Tennessee\n",
      "Country :  United States\n",
      "Zip Code :  38482\n",
      "Event : #iowacaucusdisaster\n",
      "Timeframe : from - 2020-02-04 04:46:17 to - 2020-02-04 22:59:52\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucusdisaster', 'party', 'democrats', 'iowa', 'cnn', 'dnc', 'crying', 'died', 'betrayed', 'terrence', 'caucus', '@carpedonktum']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #arsd\n",
      "Timeframe : from - 2020-02-03 23:01:05 to - 2020-02-04 22:54:08\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#arsd', '#armyselcaday', '@bts', 'twt', 'drop', '#armyseicaday', 'hype', 'arsd', '#bts', 'love', '#btsarmy', '@drwnk']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Colorado\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #yanggang\n",
      "Timeframe : from - 2020-02-03 23:01:02 to - 2020-02-04 22:58:14\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#yanggang', '@andrewyang', '#caucusforyang', 'precinct', 'iowa', 'yang', 'amp', '#iowacaucuses', 'delegates', 'caucus', 'delegate', '@indiana']\n",
      "\n",
      "\n",
      "City :  Manchester\n",
      "State :  England\n",
      "Country :  United Kingdom\n",
      "Zip Code :  M14 4DL\n",
      "Event : #giveaway\n",
      "Timeframe : from - 2020-02-03 23:00:24 to - 2020-02-04 22:54:04\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#giveaway', 'follow', '@originalfunko', 'amp', 'win', 'chance', 'retweet', 'pop', '#funko', '#funkopop', 'enter', 'giveaway']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  North Carolina\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #trump\n",
      "Timeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:58:58\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#trump', '#trumpsguilty', 'trump', '#kag', '@realdonaldtrump', '#maga', '@funder', 'landslide', '#trumpisanidiot', '#iowacaucus', '@joncoopertweets', 'hand']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #shehnaazgill\n",
      "Timeframe : from - 2020-02-03 23:02:57 to - 2020-02-04 22:55:40\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#shehnaazgill', '#sidharthshukla', '#sidnaaz', '@colorstv', '#deshkiawaazshehnaaz', '@beingsalmankhan', 'shukla', 'india', 'hai', '@vivo', 'vote', '#bb']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    event_words = exp_words_f.filter( f.col('text').contains(ht))\n",
    "    event_texts = clean_texts.filter( f.col('text').contains(ht))\n",
    "\n",
    "    ## Ex.5: Get the most frequent words for each event:\n",
    "    most_freq_w = event_words.groupBy('word') \\\n",
    "                             .count() \\\n",
    "                             .sort('count', ascending=False)\\\n",
    "                             .rdd\\\n",
    "                             .map(lambda x: x[0])\\\n",
    "                             .collect()[:12]    \n",
    "    \n",
    "    # Ex.6 : Finding timeframe of the Event\n",
    "    end   = event_texts.select(f.max(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    start = event_texts.select(f.min(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    \n",
    "    # Ex.7 : Finding location of the Event\n",
    "    coords = event_texts.na.drop(subset=[\"place\"]) \\\n",
    "                        .select(\"place.bounding_box.coordinates\") \\\n",
    "                        .rdd.map(list).collect()\n",
    "    \n",
    "    ## Gets the most common location from the tweets !\n",
    "    location_name = event_texts_na.groupBy('place.name', 'place.country') \\\n",
    "                                  .count()\\\n",
    "                                  .sort('count', ascending=False)\\\n",
    "                                  .rdd\\\n",
    "                                  .map(lambda x : x[0] + ', ' + x[1])\\\n",
    "                                  .collect()\n",
    "    \n",
    "\n",
    "    if(coords != []):\n",
    "        coords_mean = np.array(coords).squeeze().mean(axis = 0)\n",
    "        loc(coords_mean)\n",
    "    else: \n",
    "        print(\"No location specified\")\n",
    "    \n",
    "    if(most_freq_w == []):\n",
    "        most_freq_w = 'no words found...'\n",
    "    \n",
    "    if(location_name != []):\n",
    "        location_name = location_name[0]\n",
    "    else:\n",
    "        location_name = 'No Location specified'\n",
    "    \n",
    "\n",
    "    end = str( pd.to_datetime(end, unit='ms').to_pydatetime())\n",
    "    start = str(pd.to_datetime(start, unit='ms').to_pydatetime())\n",
    "    print(\"Event : \" + ht)\n",
    "    print(\"Timeframe : from - \" + start[:-7] + \" to - \" + end[:-7])\n",
    "    print(\"Most frequent Location is : \" + location_name)\n",
    "    print(\"Most frequent words associated to the event: \\n\" + str(most_freq_w) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "time spent computing: 6.778\n",
      "Method 2\n",
      "time spent computing: 6.634\n",
      "Method 3\n",
      "time spent computing: 6.558\n",
      "Method 4\n",
      "time spent computing: 0.009586\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Use describe()\n",
    "t1 = time.time()\n",
    "float(event_words.describe(\"timestamp_ms\").filter(\"summary = 'max'\").select(\"timestamp_ms\").collect()[0].asDict()['timestamp_ms'])\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 1\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 2: Use SQL\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.registerTempTable(\"df_table\")\n",
    "spark.sql(\"SELECT MAX(timestamp_ms) as maxval FROM df_table\").collect()[0].asDict()['maxval']\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 2\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 3: Convert to RDD\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(\"timestamp_ms\").rdd.max()[0]\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 3\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "\n",
    "# Method 4: select\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(f.max(f.col('timestamp_ms')))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 4\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "\n",
    "Use NLP package from [nltk](https://www.nltk.org/api/nltk.sentiment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /users/eleves-a/2018/jean-charles.layoun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis most frequent hashtags:\n",
    "\n",
    "Following the **map reduce** paradigm to compute the sentiment associated with each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Map by hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_rdd = hashtags.select('filtered', 'hashtag')\\\n",
    "                         .filter(f.col('hashtag').isin(mfht))\\\n",
    "                         .rdd.map(lambda x: (x[1], sid.polarity_scores(' '.join(x[0]))))\n",
    "\n",
    "## Because we want to reduce and aggregate with respect to each event(hashtag), we choose hashtag to be our key:\n",
    "# sentiments_rdd has (key=hashtag, value=sentiment)\n",
    "t1 = time.time()\n",
    "sentiments_rdd.collect()\n",
    "t2 = time.time()\n",
    "time1 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#superbowl', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#btsarmy', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#thebachelor', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#iowacaucuses', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#nowplaying', {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.2732}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#giveaway', {'neg': 0.0, 'neu': 0.743, 'pos': 0.257, 'compound': 0.5859})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce on Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiments(x, y):\n",
    "    dict1 = x\n",
    "    dict2 = y    \n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] += dict2[key]\n",
    "        \n",
    "    return dict1\n",
    "    \n",
    "\n",
    "aggreg = sentiments_rdd.reduceByKey(lambda x, y: aggregate_sentiments(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 4.951999999999999,\n",
       "   'neu': 1030.175000000003,\n",
       "   'pos': 196.87100000000038,\n",
       "   'compound': 349.67990000000117}),\n",
       " ('#arsd',\n",
       "  {'neg': 35.58,\n",
       "   'neu': 451.28099999999995,\n",
       "   'pos': 50.13499999999999,\n",
       "   'compound': 35.2231}),\n",
       " ('#iowa',\n",
       "  {'neg': 73.184,\n",
       "   'neu': 521.3799999999999,\n",
       "   'pos': 101.45099999999998,\n",
       "   'compound': 55.191799999999986}),\n",
       " ('#biggboss',\n",
       "  {'neg': 61.392999999999994,\n",
       "   'neu': 872.8369999999999,\n",
       "   'pos': 221.77900000000002,\n",
       "   'compound': 289.0983999999999}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 133.68300000000002,\n",
       "   'neu': 1076.398,\n",
       "   'pos': 147.925,\n",
       "   'compound': 23.1815})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "event_sentiments_l = aggreg.collect()\n",
    "t2 = time.time()\n",
    "time2 = t2-t1\n",
    "\n",
    "event_sentiments_l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentiment(x):\n",
    "    dict1    = x\n",
    "    sum_prob = dict1['neg'] + dict1['neu'] + dict1['pos']\n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] /= sum_prob\n",
    "    return dict1\n",
    "\n",
    "\n",
    "aggreg_normalized = aggreg.map(lambda x: (x[0], normalize_sentiment(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "event_sentiments_ln = aggreg_normalized.collect()\n",
    "t2 = time.time()\n",
    "time3 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 0.0040194870446218145,\n",
       "   'neu': 0.8361823639324092,\n",
       "   'pos': 0.15979814902296907,\n",
       "   'compound': 0.2838315484278386}),\n",
       " ('#arsd',\n",
       "  {'neg': 0.06625747677822555,\n",
       "   'neu': 0.8403805614939405,\n",
       "   'pos': 0.0933619617278341,\n",
       "   'compound': 0.06559285357805274}),\n",
       " ('#iowa',\n",
       "  {'neg': 0.1051471591847877,\n",
       "   'neu': 0.7490930511555067,\n",
       "   'pos': 0.1457597896597056,\n",
       "   'compound': 0.07929685423446334}),\n",
       " ('#biggboss',\n",
       "  {'neg': 0.05310771801949638,\n",
       "   'neu': 0.7550434295926761,\n",
       "   'pos': 0.19184885238782745,\n",
       "   'compound': 0.2500831740929352}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 0.09844065490137749,\n",
       "   'neu': 0.792631254942909,\n",
       "   'pos': 0.10892809015571361,\n",
       "   'compound': 0.0170702485850578}),\n",
       " ('#asimriaz',\n",
       "  {'neg': 0.06977859288060737,\n",
       "   'neu': 0.7629025185663331,\n",
       "   'pos': 0.16731888855305946,\n",
       "   'compound': 0.15989261982882397}),\n",
       " ('#curecancer',\n",
       "  {'neg': 0.2291470615972054,\n",
       "   'neu': 0.5569524341721126,\n",
       "   'pos': 0.213900504230682,\n",
       "   'compound': -0.09943524859395293}),\n",
       " ('#rashamidesai',\n",
       "  {'neg': 0.06565979381443297,\n",
       "   'neu': 0.7649621993127148,\n",
       "   'pos': 0.16937800687285218,\n",
       "   'compound': 0.17399931271477664})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_sentiments_ln[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments, aggregating and normalizing: 39.36s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments, aggregating and normalizing: {:.4g}s\".format(time1 + time2 + time3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing average sentiment on all tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_rdd  = words_f.select('filtered').rdd.map(lambda x: ' '.join(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sentences = sentences_rdd.collect()\n",
    "t2 = time.time()\n",
    "time4 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "overall_sent = {'neg': 0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "for sentence in sentences:\n",
    "    sentiment = sid.polarity_scores(sentence)    \n",
    "    for key in overall_sent.keys():\n",
    "        overall_sent[key] += sentiment[key]\n",
    "        \n",
    "    \n",
    "for key in overall_sent.keys():\n",
    "        overall_sent[key] /= num_sentences\n",
    "overall_sent\n",
    "t2 = time.time()\n",
    "time5 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments not using an rdd: 82.31s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments not using an rdd: {:.4g}s\".format(time4 + time5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "We can clearly see that using an rdd and utilizing the map reduce scheme gives us faster performance! Also, the naive algorithm above doesn't even group by hashtags..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
