{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lower, col, size, length\n",
    "from operator import add\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession\\\n",
    "  .builder \\\n",
    "  .appName(\"Twitter_app\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre',\n",
       " 'http',\n",
       " 'https']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading from the stopword file:\n",
    "text_file = open(\"datasets/stop_words_english.txt\", \"r\")\n",
    "lines = text_file.read()\n",
    "\n",
    "## Creating the stop word array:\n",
    "stopWords = lines.split()\n",
    "# Adding http and https to it:\n",
    "stopWords.append('http')\n",
    "stopWords.append('https')\n",
    "stopWords[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = spark.read.format('json').options(header='true', inferSchema='true') \\\n",
    "  .load('./datasets/NoFilterEnglish2020-02-04.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|text                                                     |\n",
      "+---------------------------------------------------------+\n",
      "|@theythemsbian thank you for being brave enough to say it|\n",
      "+---------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "clean_texts = data.select('*', lower(f.regexp_replace(f.col('text'), r'[^a-zA-Z#@,!\\\\s]', ' ')).alias('text2'))\\\n",
    "                  .drop('text')\\\n",
    "                  .withColumnRenamed('text2', 'text')\n",
    "\n",
    "clean_texts.select('text').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|words_t                                                             |\n",
      "+--------------------------------------------------------------------+\n",
      "|[@theythemsbian, thank, you, for, being, brave, enough, to, say, it]|\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words_t')\n",
    "clean_word_tokens = tokenizer.transform(clean_texts)\n",
    "\n",
    "clean_word_tokens.select('words_t').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove useless words:\n",
    "\n",
    "By useless, we mean the words of size 2 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|words                                                       |\n",
      "+------------------------------------------------------------+\n",
      "|[@theythemsbian, thank, you, for, being, brave, enough, say]|\n",
      "+------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Defining the function: (udf: user defined function)\n",
    "filter_length_udf = f.udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "clean_word_tokens = clean_word_tokens.withColumn('words', filter_length_udf(col('words_t')))\n",
    "\n",
    "clean_word_tokens.select('words').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                text|          word|\n",
      "+--------------------+--------------+\n",
      "|@theythemsbian th...|@theythemsbian|\n",
      "|@theythemsbian th...|         thank|\n",
      "|@theythemsbian th...|           you|\n",
      "|@theythemsbian th...|           for|\n",
      "+--------------------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words = clean_word_tokens.withColumn('word', f.explode('words'))\\\n",
    "                             .drop('words')\n",
    "                 \n",
    "#Tokenizer\n",
    "exp_words.select('text', 'word').show(4)\n",
    "\n",
    "exp_words.createOrReplaceTempView(\"words\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreqWords(ts1, ts2, table = \"words\"):\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT word, count(*) AS word_count \" + \n",
    "                      \"FROM {} \".format(table) + \n",
    "                      \"WHERE {0}.timestamp_ms BETWEEN {1} AND {2} \".format(table, ts1, ts2)+ \n",
    "                      \"GROUP BY word \" + \n",
    "                      \"ORDER BY word_count DESC\")\n",
    "    sqlDF.show(10)\n",
    "    return sqlDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| word|word_count|\n",
      "+-----+----------+\n",
      "|  the|        13|\n",
      "|https|         7|\n",
      "|  you|         6|\n",
      "| this|         6|\n",
      "|  can|         4|\n",
      "|  for|         4|\n",
      "| need|         3|\n",
      "| make|         3|\n",
      "|  who|         2|\n",
      "| rush|         2|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 6.498\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   13|\n",
      "|https|    7|\n",
      "|  you|    6|\n",
      "| this|    6|\n",
      "|  can|    4|\n",
      "|  for|    4|\n",
      "| make|    3|\n",
      "| need|    3|\n",
      "|  was|    2|\n",
      "|  amp|    2|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.512\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "## Timing the operation:\n",
    "t1 = time.time()\n",
    "\n",
    "exp_words.filter(f.col('timestamp_ms').between(ts1, ts2) )\\\n",
    "         .groupBy('word') \\\n",
    "         .count() \\\n",
    "         .sort('count', ascending=False) \\\n",
    "         .show(10)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "#         .filter(length(col(\"word\")) >= 3)\\\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "\n",
    "We can see that both methods are equivalent in computing time and yield to the same results. Choosing between both is just a question of taste. We Personally prefer **SQL** querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|filtered                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|[@theythemsbian, brave]                                                                                    |\n",
      "|[@camillediola, duterte, violated, law, appointing, honasan,, background,, head, dict, shrugged,, pointing]|\n",
      "|[@tinyseokjinnie, lol, bored, dypgt]                                                                       |\n",
      "|[@ibesuckafree, suck, quitting, weed, argument, amp, gettin]                                               |\n",
      "|[@mikebloomberg, donald, trump, bring, change, country, #superbowl, vciycilow]                             |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing the stopwords from the array of strings\n",
    "sc = spark.sparkContext\n",
    "broadcastVar = sc.broadcast(stopWords)\n",
    "broadcastVar.value\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=broadcastVar.value)\n",
    "words_f = remover.transform(clean_word_tokens)\n",
    "words_f = words_f.drop('words_t', 'words')\n",
    "\n",
    "words_f.select('filtered').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['truncated', 'user', 'withheld_in_countries', 'text', 'filtered']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words_filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+--------------+\n",
      "|text                                                     |word          |\n",
      "+---------------------------------------------------------+--------------+\n",
      "|@theythemsbian thank you for being brave enough to say it|@theythemsbian|\n",
      "+---------------------------------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words_f = words_f.withColumn('word', f.explode('filtered'))#\\\n",
    "                     #.drop('filtered')\n",
    "#.drop('filtered')???\n",
    "\n",
    "exp_words_f.select('text', 'word').show(1, False)\n",
    "\n",
    "exp_words_f.createOrReplaceTempView(\"words_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'withheld_in_countries', 'text', 'filtered', 'word']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time after filtering using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     word|word_count|\n",
      "+---------+----------+\n",
      "|      amp|         2|\n",
      "|     rare|         2|\n",
      "|     rush|         2|\n",
      "|      pro|         2|\n",
      "|     fuck|         2|\n",
      "| whaaaaat|         1|\n",
      "|    dypgt|         1|\n",
      "|   hyolyn|         1|\n",
      "|@santeira|         1|\n",
      "|     guys|         1|\n",
      "+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.862\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words_filtered\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|            word|count|\n",
      "+----------------+-----+\n",
      "|             don| 7919|\n",
      "|             amp| 7470|\n",
      "|          people| 6621|\n",
      "|            love| 6304|\n",
      "|            time| 5231|\n",
      "|             day| 4775|\n",
      "|           trump| 4621|\n",
      "|            good| 4434|\n",
      "|           today| 3708|\n",
      "|            iowa| 3488|\n",
      "|            shit| 3213|\n",
      "|             man| 3156|\n",
      "|            life| 2527|\n",
      "|           happy| 2424|\n",
      "|            fuck| 2337|\n",
      "|            year| 2273|\n",
      "|           years| 2206|\n",
      "|           black| 2127|\n",
      "|@realdonaldtrump| 2107|\n",
      "|           great| 2065|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words_f.filter(\"(timestamp_ms / 1000 / 60 / 60  % 24 )>= 20\") \\\n",
    "           .groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = exp_words_f.filter( exp_words_f.word.startswith('#'))\\\n",
    "                      .withColumnRenamed('word', 'hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             hashtag|count|\n",
      "+--------------------+-----+\n",
      "|         #curecancer| 3987|\n",
      "|       #iowacaucuses| 3184|\n",
      "|#deshkiawaazshehnaaz| 2867|\n",
      "|    #mainbhiasimkigf| 2772|\n",
      "|      #meraadarshsid| 2681|\n",
      "|       #iheartawards| 1713|\n",
      "|                #bts| 1654|\n",
      "|           #asimriaz| 1494|\n",
      "|        #thebachelor| 1353|\n",
      "|                 #bb| 1251|\n",
      "|         #iowacaucus| 1249|\n",
      "|     #sidharthshukla| 1235|\n",
      "|           #treasure| 1232|\n",
      "|        #coronavirus| 1155|\n",
      "|           #biggboss| 1155|\n",
      "|           #iacaucus| 1134|\n",
      "|         #loveisland|  996|\n",
      "|          #superbowl|  962|\n",
      "|     #worldcancerday|  863|\n",
      "|               #sotu|  842|\n",
      "|     #rightchoicesid|  839|\n",
      "|              #yikes|  758|\n",
      "|     #happyjisungday|  706|\n",
      "|         #nowplaying|  646|\n",
      "|            #btsarmy|  646|\n",
      "|                #nct|  640|\n",
      "|             #superm|  573|\n",
      "|               #iowa|  572|\n",
      "|                #raw|  565|\n",
      "|       #rashamidesai|  564|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_freq_hash = hashtags.groupBy('hashtag') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False) \n",
    "most_freq_hash.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting Hashtag Data Frame to array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfht = most_freq_hash.filter('count >= 500').select('hashtag').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting words related to Hashtag example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#iowacaucuses',\n",
       " 'iowa',\n",
       " 'bernie',\n",
       " 'caucus',\n",
       " 'vote',\n",
       " 'won',\n",
       " 'tonight',\n",
       " '#iowacaucus',\n",
       " 'biden',\n",
       " 'app',\n",
       " '@donaldjtrumpjr',\n",
       " 'dnc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_words = exp_words_f.filter( f.col('text').contains('#iowacaucuses'))\n",
    "\n",
    "event_words.groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False)\\\n",
    "           .rdd\\\n",
    "           .map(lambda x: x[0])\\\n",
    "           .collect()[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex5 using pySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----+\n",
      "|        hashtag|       word|count|\n",
      "+---------------+-----------+-----+\n",
      "|         #yikes|@nickiminaj|  451|\n",
      "|         #yikes|      likes|  344|\n",
      "|         #yikes|        amp|  336|\n",
      "|         #yikes|       play|  325|\n",
      "|         #yikes|        tag|  313|\n",
      "|         #yikes|       life|  311|\n",
      "|         #yikes|    yzdifvf|  310|\n",
      "|         #yikes|       rosa|  130|\n",
      "|         #yikes|      nicki|  111|\n",
      "|#worldcancerday|     cancer|  347|\n",
      "+---------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ex.5: Get the most frequent words for each event:\n",
    "ht_f = hashtags.select('filtered', 'hashtag')\\\n",
    "               .filter(f.col('hashtag').isin(mfht))\\\n",
    "               .withColumn('word', f.explode('filtered'))\\\n",
    "               .filter(~col('word').startswith('#'))\\\n",
    "               .select('hashtag', 'word')\\\n",
    "               .groupBy('hashtag', 'word')\\\n",
    "               .count()\\\n",
    "               .sort(['hashtag', 'count'], ascending=False)\\\n",
    "               .filter('count >= 100')\n",
    "\n",
    "ht_f.show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the top 10 most frequent words by events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|    hashtag|       word|count|\n",
      "+-----------+-----------+-----+\n",
      "|#curecancer|trueworship| 3951|\n",
      "|#curecancer|     cancer| 2441|\n",
      "|#curecancer|       true| 1240|\n",
      "|#curecancer|     rampal| 1073|\n",
      "|#curecancer|    worship| 1042|\n",
      "|#curecancer|      saint| 1040|\n",
      "|#curecancer|       cure|  916|\n",
      "|#curecancer|        god|  884|\n",
      "|#curecancer|    maharaj|  859|\n",
      "|#curecancer|   diseases|  846|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+---------------+-----+\n",
      "|      hashtag|           word|count|\n",
      "+-------------+---------------+-----+\n",
      "|#iowacaucuses|           iowa|  499|\n",
      "|#iowacaucuses|         bernie|  346|\n",
      "|#iowacaucuses|         caucus|  237|\n",
      "|#iowacaucuses|           vote|  193|\n",
      "|#iowacaucuses|            won|  173|\n",
      "|#iowacaucuses|        tonight|  155|\n",
      "|#iowacaucuses|          biden|  153|\n",
      "|#iowacaucuses|            app|  151|\n",
      "|#iowacaucuses|@donaldjtrumpjr|  150|\n",
      "|#iowacaucuses|            dnc|  147|\n",
      "+-------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+-----+\n",
      "|             hashtag|     word|count|\n",
      "+--------------------+---------+-----+\n",
      "|#deshkiawaazshehnaaz|     sana|  459|\n",
      "|#deshkiawaazshehnaaz|     love|  287|\n",
      "|#deshkiawaazshehnaaz|     guys|  235|\n",
      "|#deshkiawaazshehnaaz| shehnaaz|  193|\n",
      "|#deshkiawaazshehnaaz|    trend|  143|\n",
      "|#deshkiawaazshehnaaz|@colorstv|  141|\n",
      "|#deshkiawaazshehnaaz|    speed|  128|\n",
      "|#deshkiawaazshehnaaz| trending|  119|\n",
      "|#deshkiawaazshehnaaz|   follow|  116|\n",
      "|#deshkiawaazshehnaaz|      hai|  113|\n",
      "+--------------------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-----------+-----+\n",
      "|         hashtag|       word|count|\n",
      "+----------------+-----------+-----+\n",
      "|#mainbhiasimkigf|       asim|  982|\n",
      "|#mainbhiasimkigf|      trend|  485|\n",
      "|#mainbhiasimkigf|      girls|  482|\n",
      "|#mainbhiasimkigf|   trending|  291|\n",
      "|#mainbhiasimkigf|  @colorstv|  253|\n",
      "|#mainbhiasimkigf|       love|  214|\n",
      "|#mainbhiasimkigf|   fangirls|  202|\n",
      "|#mainbhiasimkigf|        fan|  193|\n",
      "|#mainbhiasimkigf|  @biggboss|  187|\n",
      "|#mainbhiasimkigf|@imrealasim|  186|\n",
      "+----------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+---------------+-----+\n",
      "|       hashtag|           word|count|\n",
      "+--------------+---------------+-----+\n",
      "|#meraadarshsid|            sid|  516|\n",
      "|#meraadarshsid|         shukla|  428|\n",
      "|#meraadarshsid|      @sidharth|  352|\n",
      "|#meraadarshsid|         tweets|  282|\n",
      "|#meraadarshsid|@realvindusingh|  281|\n",
      "|#meraadarshsid|          today|  228|\n",
      "|#meraadarshsid|            tag|  223|\n",
      "|#meraadarshsid|      @biggboss|  180|\n",
      "|#meraadarshsid|       activity|  162|\n",
      "|#meraadarshsid|        tagline|  160|\n",
      "+--------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+----------------+-----+\n",
      "|      hashtag|            word|count|\n",
      "+-------------+----------------+-----+\n",
      "|#iheartawards|            @bts|  567|\n",
      "|#iheartawards|             twt|  553|\n",
      "|#iheartawards|            vote|  536|\n",
      "|#iheartawards|             sos|  264|\n",
      "|#iheartawards|   @justinbieber|  259|\n",
      "|#iheartawards|     @danandshay|  226|\n",
      "|#iheartawards|            @tha|  220|\n",
      "|#iheartawards|@ygofficialblink|  216|\n",
      "|#iheartawards|          voting|  200|\n",
      "|#iheartawards|           hours|  187|\n",
      "+-------------+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+----------+-----+\n",
      "|hashtag|      word|count|\n",
      "+-------+----------+-----+\n",
      "|   #bts|      @bts|  739|\n",
      "|   #bts|       twt|  525|\n",
      "|   #bts|      soul|  403|\n",
      "|   #bts|     outro|  368|\n",
      "|   #bts|  comeback|  357|\n",
      "|   #bts|   trailer|  355|\n",
      "|   #bts|@bighitent|  305|\n",
      "|   #bts|       map|  297|\n",
      "|   #bts|       ego|  225|\n",
      "|   #bts|     weeks|  218|\n",
      "+-------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+----+-----+\n",
      "|  hashtag|word|count|\n",
      "+---------+----+-----+\n",
      "|#asimriaz|asim|  334|\n",
      "|#asimriaz|fans|  218|\n",
      "|#asimriaz| amp|  208|\n",
      "|#asimriaz|love|  104|\n",
      "+---------+----+-----+\n",
      "\n",
      "+------------+------+-----+\n",
      "|     hashtag|  word|count|\n",
      "+------------+------+-----+\n",
      "|#thebachelor| peter|  286|\n",
      "|#thebachelor|kelley|  130|\n",
      "|#thebachelor|kelsey|  128|\n",
      "|#thebachelor| tammy|  113|\n",
      "+------------+------+-----+\n",
      "\n",
      "+-----------+------+-----+\n",
      "|    hashtag|  word|count|\n",
      "+-----------+------+-----+\n",
      "|#iowacaucus|  iowa|  226|\n",
      "|#iowacaucus|bernie|  148|\n",
      "|#iowacaucus|caucus|  127|\n",
      "+-----------+------+-----+\n",
      "\n",
      "+-------+---------+-----+\n",
      "|hashtag|     word|count|\n",
      "+-------+---------+-----+\n",
      "|    #bb|   shukla|  211|\n",
      "|    #bb|      sid|  134|\n",
      "|    #bb|@sidharth|  125|\n",
      "|    #bb|     asim|  121|\n",
      "|    #bb|@colorstv|  105|\n",
      "|    #bb|      amp|  104|\n",
      "+-------+---------+-----+\n",
      "\n",
      "+---------------+---------+-----+\n",
      "|        hashtag|     word|count|\n",
      "+---------------+---------+-----+\n",
      "|#sidharthshukla|      amp|  206|\n",
      "|#sidharthshukla|   shukla|  187|\n",
      "|#sidharthshukla|@sidharth|  147|\n",
      "|#sidharthshukla|      sid|  137|\n",
      "|#sidharthshukla|      hai|  104|\n",
      "+---------------+---------+-----+\n",
      "\n",
      "+---------+-----------+-----+\n",
      "|  hashtag|       word|count|\n",
      "+---------+-----------+-----+\n",
      "|#treasure|  editorial| 2316|\n",
      "|#treasure|   treasure| 1178|\n",
      "|#treasure|       week| 1160|\n",
      "|#treasure|      maker| 1160|\n",
      "|#treasure|photography| 1158|\n",
      "|#treasure|   official| 1148|\n",
      "|#treasure|     @ygent| 1148|\n",
      "|#treasure|       bang|  373|\n",
      "|#treasure|        dam|  373|\n",
      "|#treasure|      asahi|  256|\n",
      "+---------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+--------------+-----+\n",
      "|  hashtag|          word|count|\n",
      "+---------+--------------+-----+\n",
      "|#iacaucus|          iowa|  221|\n",
      "|#iacaucus| @andrewgillum|  141|\n",
      "|#iacaucus|@gopchairwoman|  139|\n",
      "|#iacaucus|          wait|  129|\n",
      "|#iacaucus|     precincts|  122|\n",
      "|#iacaucus|        delays|  121|\n",
      "|#iacaucus|          hear|  115|\n",
      "|#iacaucus|           bad|  115|\n",
      "|#iacaucus|        winner|  113|\n",
      "|#iacaucus|         black|  113|\n",
      "+---------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+---------+-----+\n",
      "|     hashtag|     word|count|\n",
      "+------------+---------+-----+\n",
      "|#coronavirus|    china|  238|\n",
      "|#coronavirus|     evil|  153|\n",
      "|#coronavirus|    cases|  133|\n",
      "|#coronavirus|confirmed|  103|\n",
      "+------------+---------+-----+\n",
      "\n",
      "+---------+---------+-----+\n",
      "|  hashtag|     word|count|\n",
      "+---------+---------+-----+\n",
      "|#biggboss|     asim|  173|\n",
      "|#biggboss|@colorstv|  151|\n",
      "|#biggboss|     boss|  134|\n",
      "|#biggboss|      amp|  131|\n",
      "|#biggboss|     fans|  131|\n",
      "|#biggboss|     bigg|  130|\n",
      "|#biggboss|     riaz|  129|\n",
      "|#biggboss|   shukla|  120|\n",
      "+---------+---------+-----+\n",
      "\n",
      "+-----------+--------+-----+\n",
      "|    hashtag|    word|count|\n",
      "+-----------+--------+-----+\n",
      "|#loveisland|  callum|  240|\n",
      "|#loveisland|shaughna|  117|\n",
      "+-----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-18df6455be52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mht\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmfht\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mht_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hashtag'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    ht_f.filter(col('hashtag').isin(ht)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 6-7 with SparkRDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because we want to reduce and aggregate with respect to each event(hashtag), we choose hashtag to be our key:\n",
    "# ht_rdd has (key=hashtag, value=(min(timestamp_ms), max(timestamp_ms), place.bounding_box.coordinates, number_of_places))\n",
    "ht_rdd = hashtags.select('timestamp_ms', 'place.bounding_box.coordinates', 'hashtag')\\\n",
    "                 .filter(f.col('hashtag').isin(mfht))\\\n",
    "                 .rdd.map(lambda x: (x[2], (x[0], x[0], x[1], 0 if x[1] is None else 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate function for finding minimum, maximum timeframe and the mean coordinates of each Event    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ts_place(x, y):\n",
    "    # for calculating minimum timeframe\n",
    "    ts1 = int(x[0])\n",
    "    ts2 = int(y[0])\n",
    "    # for calculating minimum timeframe    \n",
    "    ts3 = int(x[1])\n",
    "    ts4 = int(y[1])\n",
    "    \n",
    "    # for summing the coordinates\n",
    "    place1 = x[2]\n",
    "    place2 = y[2]\n",
    "    \n",
    "    # number of places of Event, needed for calculating the mean of the coordinates\n",
    "    increment = 0\n",
    "    \n",
    "    # Considering that most of the Tweets don't have the coordinates\n",
    "    # Aggregate function will select:\n",
    "    # - None, in case both of tweets do not have\n",
    "    if(place1 is None):\n",
    "        if(place2 is None):\n",
    "            place = None\n",
    "    # - Coordinates of one, if only one of the has  \n",
    "        else:\n",
    "            place = place2\n",
    "            increment = y[3]\n",
    "        \n",
    "    elif(place2 is None):\n",
    "        place = place1\n",
    "        increment = x[3]\n",
    "    # - Sum of the coordinates if both of them have\n",
    "    else:\n",
    "        place = np.array(place1) + np.array(place2)\n",
    "        place = place.squeeze()\n",
    "        increment = x[3] + y[3]\n",
    "    # Finally returns, min/max timeframe, coordinates of the place, and number of places for each Event        \n",
    "    return (min(ts3, ts4), max(ts1, ts2), place, increment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We know need to compute the means of all places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the Aggregation function\n",
    "aggreg_ht = ht_rdd.reduceByKey(lambda x, y: aggregate_ts_place(x, y))\n",
    "aggreg_htn = aggreg_ht.map(lambda x: (x[0], x[1][0], x[1][1], np.array(x[1][2])/x[1][3] if x[1][3] != 0 else x[1][3], x[1][3]))\n",
    "ts_loc_l = aggreg_htn.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT: #treasure\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:21:02 to - 2020-02-04 22:47:25\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #arsd\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 21:54:36 to - 2020-02-04 22:54:08\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowa\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:33:04 to - 2020-02-04 22:57:14\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Oklahoma\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #biggboss\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:17 to - 2020-02-04 22:51:09\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Telangana\n",
      "\tCountry :  India\n",
      "\tZip Code :  500089\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #thebachelor\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:18:15 to - 2020-02-04 22:35:01\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Kentucky\n",
      "\tCountry :  United States\n",
      "\tZip Code :  42633\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #asimriaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:11 to - 2020-02-04 22:54:44\n",
      "\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #curecancer\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:26:22 to - 2020-02-04 22:40:42\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Madhya Pradesh\n",
      "\tCountry :  India\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #rashamidesai\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:29:18 to - 2020-02-04 22:54:19\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:55:19 to - 2020-02-04 22:59:43\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Indiana\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #sidharthshukla\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:25:15 to - 2020-02-04 22:55:59\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #bts\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:59:09 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Virginia\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #raw\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:18:30 to - 2020-02-04 22:45:35\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #rightchoicesid\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 20:34:15 to - 2020-02-04 21:47:09\n",
      "\n",
      "\tCity :  Karnal\n",
      "\tState :  Haryana\n",
      "\tCountry :  India\n",
      "\tZip Code :  132001\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #coronavirus\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:58:49 to - 2020-02-04 22:59:29\n",
      "\n",
      "\tCity :  \n",
      "\tState :  كوركول\n",
      "\tCountry :  موريتانيا\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #nct\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:54:59 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #superbowl\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:29:25 to - 2020-02-04 22:46:36\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowacaucuses\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:41 to - 2020-02-04 22:57:53\n",
      "\n",
      "\tCity :  \n",
      "\tState :  West Virginia\n",
      "\tCountry :  United States\n",
      "\tZip Code :  25270\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #nowplaying\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:30:38 to - 2020-02-04 22:55:34\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #worldcancerday\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:28:44 to - 2020-02-04 22:53:51\n",
      "\n",
      "\tCity :  Sidi Ghanem سيدي غانم\n",
      "\tState :  \n",
      "\tCountry :  Maroc / ⵍⵎⵖⵔⵉⴱ / المغرب\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowacaucusdisaster\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:57:07 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Arkansas\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iheartawards\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:33:44 to - 2020-02-04 22:58:24\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:25 to - 2020-02-04 22:57:49\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  65274\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #superm\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:05 to - 2020-02-04 22:53:47\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #yikes\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:28:22 to - 2020-02-04 22:54:43\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #meraadarshsid\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:17 to - 2020-02-04 22:58:41\n",
      "\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #loveisland\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:58:52 to - 2020-02-04 22:59:25\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #sb\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:24:20 to - 2020-02-04 22:58:54\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #shehnaazgill\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:17:44 to - 2020-02-04 22:46:59\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #btsarmy\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:29:35 to - 2020-02-04 22:58:24\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Wisconsin\n",
      "\tCountry :  United States\n",
      "\tZip Code :  54241\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #mainbhiasimkigf\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:33:58 to - 2020-02-04 22:44:25\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #deshkiawaazshehnaaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:30:36 to - 2020-02-04 22:56:56\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  431201\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #trump\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:00 to - 2020-02-04 22:57:45\n",
      "\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  27540\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #sotu\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:58:40 to - 2020-02-04 22:59:37\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #happyjisungday\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:59:01 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #bb\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:23 to - 2020-02-04 22:56:17\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Telangana\n",
      "\tCountry :  India\n",
      "\tZip Code :  500089\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for event in ts_loc_l:\n",
    "    print(\"EVENT: \" + event[0]+'\\n')\n",
    "    start = str(pd.to_datetime(event[1], unit='ms').to_pydatetime())\n",
    "    end = str( pd.to_datetime(event[2], unit='ms').to_pydatetime())\n",
    "    print(\"\\tTimeframe : from - \" + start[:-7] + \" to - \" + end[:-7]+'\\n')\n",
    "    \n",
    "    if(np.sum(event[4]) == 0 ):\n",
    "        print(\"\\tNo location specified\")\n",
    "    else: \n",
    "        loc(event[3].squeeze())\n",
    "    print(\"\\n-----------------------------------------------------------------------\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that converts geolocalisation to place name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(coords):\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")  \n",
    "    if coords.shape != (2,):\n",
    "        coords = coords.mean(axis = 0)\n",
    "    \n",
    "    Longitude = str(coords[0])\n",
    "    Latitude = str(coords[1])\n",
    "    \n",
    "    location = geolocator.reverse(Latitude+\",\"+Longitude)\n",
    "    \n",
    "    if(location == None):\n",
    "        print(\"\\tNo location specified\")\n",
    "        return\n",
    " \n",
    "    address = location.raw['address']\n",
    "\n",
    "    # traverse the data\n",
    "    city = address.get('city', '')\n",
    "    state = address.get('state', '')\n",
    "    country = address.get('country', '')\n",
    "    code = address.get('country_code')\n",
    "    zipcode = address.get('postcode')\n",
    "    print('\\tCity : ', city)\n",
    "    print('\\tState : ', state)\n",
    "    print('\\tCountry : ', country)\n",
    "    print('\\tZip Code : ', zipcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 and Exercice 7: with for loop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: #curecancer\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 03:16:13 to - 2020-02-04 22:42:39\n",
      "\n",
      "\tMost frequent Location is : karnal, India\n",
      "\tCity :  \n",
      "\tState :  Madhya Pradesh\n",
      "\tCountry :  India\n",
      "\tZip Code :  476332\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['trueworship', 'cancer', 'true', 'rampal', 'saint', 'worship', 'cure', 'god']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowacaucuses\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:58:28\n",
      "\n",
      "\tMost frequent Location is : Manhattan, United States\n",
      "\tCity :  \n",
      "\tState :  West Virginia\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', 'bernie', 'caucus', 'vote', 'won', 'app', '@donaldjtrumpjr', 'party']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #deshkiawaazshehnaaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 01:58:49 to - 2020-02-04 22:59:12\n",
      "\n",
      "\tMost frequent Location is : Aurangabad, India\n",
      "\tCity :  \n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  431201\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['sana', 'love', 'guys', 'shehnaaz', 'trend', '@colorstv', 'speed', 'follow']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #mainbhiasimkigf\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:26:18 to - 2020-02-04 22:58:06\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['asim', 'trend', 'girls', 'trending', '@colorstv', 'love', 'fangirls', 'fan']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #meraadarshsid\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 08:30:32 to - 2020-02-04 22:59:16\n",
      "\n",
      "\tMost frequent Location is : Mumbai, India\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['sid', 'shukla', '@sidharth', '@realvindusingh', 'tweets', 'tag', 'today', '@biggboss']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iheartawards\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:38 to - 2020-02-04 22:58:29\n",
      "\n",
      "\tMost frequent Location is : Sheffield, United Kingdom\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'vote', 'sos', '@justinbieber', '@danandshay', '@tha', '@ygofficialblink']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #bts\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:59:54\n",
      "\n",
      "\tMost frequent Location is : Miami Shores, United States\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'bts', 'soul', 'outro', 'comeback', 'trailer', '@bighitent']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #asimriaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:59:07\n",
      "\n",
      "\tMost frequent Location is : Mumbai, India\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['asim', 'amp', 'fans', 'love', 'vote', 'media', '@biggboss', '@thekhbri']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #thebachelor\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:54:09\n",
      "\n",
      "\tMost frequent Location is : Austin, United States\n",
      "\tCity :  \n",
      "\tState :  Kentucky\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['peter', 'kelley', 'kelsey', 'tammy', 'mykenna', 'drama', 'bachelor', 'girls']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #bb\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:05:20 to - 2020-02-04 22:58:37\n",
      "\n",
      "\tMost frequent Location is : South Africa, South Africa\n",
      "\tCity :  \n",
      "\tState :  Kogi\n",
      "\tCountry :  Nigeria\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['shukla', 'prior', 'pyong', '@cleytu', 'tweet', 'appreciation', 'thnbpfwynp', 'jantando']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tMost frequent Location is : Des Moines, United States\n",
      "\tCity :  \n",
      "\tState :  Ohio\n",
      "\tCountry :  United States\n",
      "\tZip Code :  45662\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', 'bernie', 'caucus', 'app', 'breaking', 'vote', 'democrats', 'party']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #sidharthshukla\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:15:22 to - 2020-02-04 22:56:29\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['amp', 'shukla', '@sidharth', 'sid', 'hai', 'sidharth', 'boss', 'bigg']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #treasure\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:44 to - 2020-02-04 22:47:25\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['editorial', 'treasure', 'week', 'maker', 'photography', 'official', '@ygent', 'bang']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #coronavirus\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:59:29\n",
      "\n",
      "\tMost frequent Location is : Honolulu, United States\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['china', 'evil', 'cases', 'chinese', 'confirmed', 'medical', 'patients', 'hospital']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #biggboss\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:56:29\n",
      "\n",
      "\tMost frequent Location is : Rajendra Nagar, India\n",
      "\tCity :  \n",
      "\tState :  Telangana\n",
      "\tCountry :  India\n",
      "\tZip Code :  500089\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['asim', '@colorstv', 'amp', 'fans', 'riaz', 'boss', 'bigg', 'shukla']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:04:16 to - 2020-02-04 22:59:04\n",
      "\n",
      "\tMost frequent Location is : Des Moines, United States\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', '@andrewgillum', '@gopchairwoman', 'wait', 'precincts', 'delays', 'hear', 'winner']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #loveisland\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:30 to - 2020-02-04 22:59:25\n",
      "\n",
      "\tMost frequent Location is : Brent, United Kingdom\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['callum', 'shaughna', 'casa', 'mike', 'amor', 'ched', 'villa', 'girl']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #superbowl\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 22:59:55 to - 2020-02-04 22:52:53\n",
      "\n",
      "\tMost frequent Location is : Houston, United States\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['trump', 'anthem', 'national', 'halftime', 'mocking', '@shakira', 'stand', 'caught']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #worldcancerday\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:29:10 to - 2020-02-04 22:58:40\n",
      "\n",
      "\tMost frequent Location is : God's Favourite House, Nigeria\n",
      "\tCity :  Sidi Ghanem سيدي غانم\n",
      "\tState :  \n",
      "\tCountry :  Maroc / ⵍⵎⵖⵔⵉⴱ / المغرب\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['cancer', 'today', 'amp', 'people', 'day', 'awareness', 'support', 'disease']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #sotu\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:08:18 to - 2020-02-04 22:59:43\n",
      "\n",
      "\tMost frequent Location is : Florida, United States\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['tomorrow', 'trump', 'attend', 'tonight', 'join', 'people', '@potus', '@flotus']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: #rightchoicesid\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:39 to - 2020-02-04 21:47:09\n",
      "\n",
      "\tMost frequent Location is : karnal, India\n",
      "\tCity :  Karnal\n",
      "\tState :  Haryana\n",
      "\tCountry :  India\n",
      "\tZip Code :  132001\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['sid', 'shukla', '@sidharth', '@realvindusingh', 'guys', 'trend', 'win', '@biggboss']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #yikes\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 03:15:25 to - 2020-02-04 22:57:35\n",
      "\n",
      "\tMost frequent Location is : Bryn Mawr, United States\n",
      "\tCity :  \n",
      "\tState :  Kansas\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@nickiminaj', 'likes', 'amp', 'play', 'life', 'tag', 'yzdifvf', 'nicki']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #happyjisungday\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 03:27:07 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['birthday', 'happy', '@nctsmtown', 'slthg', 'jisung', 'dream', 'park', 'baby']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #nowplaying\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:17 to - 2020-02-04 22:56:54\n",
      "\n",
      "\tMost frequent Location is : Potsdam, Deutschland\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['listen', 'radio', 'live', 'amp', 'tune', 'love', 'feat', 'music']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #btsarmy\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:58:56\n",
      "\n",
      "\tMost frequent Location is : Two Rivers, United States\n",
      "\tCity :  \n",
      "\tState :  Wisconsin\n",
      "\tCountry :  United States\n",
      "\tZip Code :  54241\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'message', 'bts', '@choi', 'album', 'era', '@btsvotingteam']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #nct\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:09:25 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['birthday', 'happy', '@nctsmtown', 'slthg', 'dream', 'icn', 'superm', 'live']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #superm\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:58 to - 2020-02-04 22:58:32\n",
      "\n",
      "\tMost frequent Location is : Maryland, United States\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  27239:27292\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['live', '@superm', 'superm', 'san', 'future', 'jose', 'sjc', 'hhelen']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowa\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tMost frequent Location is : Des Moines, United States\n",
      "\tCity :  \n",
      "\tState :  Kentucky\n",
      "\tCountry :  United States\n",
      "\tZip Code :  41006\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', 'bernie', 'caucus', 'app', 'democrats', 'breaking', 'vote', 'party']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #raw\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:07:33 to - 2020-02-04 22:56:17\n",
      "\n",
      "\tMost frequent Location is : Mayville, United States\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@wwe', 'wwe', '@wweuniverse', '@randyorton', '@wweasuka', 'challenge', '@angelgarzawwe', '@beckylynchwwe']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #rashamidesai\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:03:45 to - 2020-02-04 22:58:18\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['amp', 'khabri', '@biggboss', '@real', '@therashamidesai', 'elite', 'breaking', 'rashami']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #sb\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:37 to - 2020-02-04 22:58:54\n",
      "\n",
      "\tMost frequent Location is : Armagh, United Kingdom\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@sb', 'official', 'billboardmainstay', '@mor', 'alab', 'onawesamsungcon', '@nfl', '@jlo']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #arsd\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:05 to - 2020-02-04 22:54:08\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'drop', 'hype', 'arsd', 'love', '@drwnk', 'zvrnb']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowacaucusdisaster\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 04:46:17 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tMost frequent Location is : Corinth, United States\n",
      "\tCity :  \n",
      "\tState :  Tennessee\n",
      "\tCountry :  United States\n",
      "\tZip Code :  38482\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['democrats', 'iowa', 'cnn', 'party', 'dnc', 'crying', 'died', 'betrayed']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #shehnaazgill\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:02:57 to - 2020-02-04 22:55:40\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@colorstv', '@beingsalmankhan', 'shukla', 'india', 'hai', '@vivo', 'vote', 'asim']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #trump\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:58:58\n",
      "\n",
      "\tMost frequent Location is : Antioch, United States\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['trump', '@realdonaldtrump', '@funder', 'landslide', '@joncoopertweets', 'hand', 'raise', 'schiff']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    event_words = exp_words_f.filter( f.col('text').contains(ht))\n",
    "    event_texts = clean_texts.filter( f.col('text').contains(ht))\n",
    "\n",
    "    ## Ex.5: Get the most frequent words for each event:\n",
    "    most_freq_w = event_words.filter(~col('word').startswith('#'))\\\n",
    "                             .groupBy('word') \\\n",
    "                             .count() \\\n",
    "                             .sort('count', ascending=False)\\\n",
    "                             .rdd\\\n",
    "                             .map(lambda x: x[0])\\\n",
    "                             .collect()[:8]    \n",
    "    \n",
    "    # Ex.6 : Finding timeframe of the Event\n",
    "    end   = event_texts.select(f.max(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    start = event_texts.select(f.min(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    \n",
    "    # Ex.7 : Finding location of the Event\n",
    "    coords = event_texts.na.drop(subset=[\"place\"]) \\\n",
    "                        .select(\"place.bounding_box.coordinates\") \\\n",
    "                        .rdd.map(list).collect()\n",
    "    \n",
    "    ## Gets the most common location from the tweets !\n",
    "    location_name = event_texts.na.drop(subset=[\"place\"]) \\\n",
    "                               .groupBy('place.name', 'place.country') \\\n",
    "                               .count()\\\n",
    "                               .sort('count', ascending=False)\\\n",
    "                               .rdd\\\n",
    "                               .map(lambda x : x[0] + ', ' + x[1])\\\n",
    "                               .collect()\n",
    "    \n",
    "    ####### Printing the results\n",
    "    \n",
    "    print(\"EVENT: \" + ht+'\\n')\n",
    "    \n",
    "    end = str( pd.to_datetime(end, unit='ms').to_pydatetime())\n",
    "    start = str(pd.to_datetime(start, unit='ms').to_pydatetime())\n",
    "    print(\"\\tTimeframe : from - \" + start[:-7] + \" to - \" + end[:-7]+'\\n')\n",
    "    \n",
    "    # Printing the location found by the first method: The Most Common Location\n",
    "    if(location_name != []):\n",
    "        location_name = location_name[0]\n",
    "    else:\n",
    "        location_name = 'No Location specified'\n",
    "\n",
    "    print(\"\\tMost frequent Location is : \" + location_name)\n",
    "    \n",
    "    # Printing the location found by the second method: The Mean Of Coordinates\n",
    "    if(coords != []):\n",
    "        coords_mean = np.array(coords).squeeze().mean(axis = 0)\n",
    "        loc(coords_mean)\n",
    "    else: \n",
    "        print(\"\\tNo location specified\")\n",
    "    \n",
    "    # Printing most frequent words\n",
    "    if(most_freq_w == []):\n",
    "        most_freq_w = 'no words found...'\n",
    "    \n",
    "\n",
    "    print(\"\\n\\tMost frequent words associated to the event: \\n\\t\" + str(most_freq_w))\n",
    "    print(\"\\n------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Benchmarking of the different methods for finding minimum / maximum timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "time spent computing: 6.778\n",
      "Method 2\n",
      "time spent computing: 6.634\n",
      "Method 3\n",
      "time spent computing: 6.558\n",
      "Method 4\n",
      "time spent computing: 0.009586\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Use describe()\n",
    "t1 = time.time()\n",
    "float(event_words.describe(\"timestamp_ms\").filter(\"summary = 'max'\").select(\"timestamp_ms\").collect()[0].asDict()['timestamp_ms'])\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 1\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 2: Use SQL\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.registerTempTable(\"df_table\")\n",
    "spark.sql(\"SELECT MAX(timestamp_ms) as maxval FROM df_table\").collect()[0].asDict()['maxval']\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 2\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 3: Convert to RDD\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(\"timestamp_ms\").rdd.max()[0]\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 3\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "\n",
    "# Method 4: select\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(f.max(f.col('timestamp_ms')))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 4\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "\n",
    "Use NLP package from [nltk](https://www.nltk.org/api/nltk.sentiment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /users/eleves-a/2018/jean-charles.layoun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis most frequent hashtags:\n",
    "\n",
    "Following the **map reduce** paradigm to compute the sentiment associated with each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Map by hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_rdd = hashtags.select('filtered', 'hashtag')\\\n",
    "                         .filter(f.col('hashtag').isin(mfht))\\\n",
    "                         .rdd.map(lambda x: (x[1], sid.polarity_scores(' '.join(x[0]))))\n",
    "\n",
    "## Because we want to reduce and aggregate with respect to each event(hashtag), we choose hashtag to be our key:\n",
    "# sentiments_rdd has (key=hashtag, value=sentiment)\n",
    "t1 = time.time()\n",
    "sentiments_rdd.collect()\n",
    "t2 = time.time()\n",
    "time1 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#superbowl', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#btsarmy', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#thebachelor', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#iowacaucuses', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#nowplaying', {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.2732}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#giveaway', {'neg': 0.0, 'neu': 0.743, 'pos': 0.257, 'compound': 0.5859})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce on Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiments(x, y):\n",
    "    dict1 = x\n",
    "    dict2 = y    \n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] += dict2[key]\n",
    "        \n",
    "    return dict1\n",
    "    \n",
    "\n",
    "aggreg = sentiments_rdd.reduceByKey(lambda x, y: aggregate_sentiments(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 4.951999999999999,\n",
       "   'neu': 1030.175000000003,\n",
       "   'pos': 196.87100000000038,\n",
       "   'compound': 349.67990000000117}),\n",
       " ('#arsd',\n",
       "  {'neg': 35.58,\n",
       "   'neu': 451.28099999999995,\n",
       "   'pos': 50.13499999999999,\n",
       "   'compound': 35.2231}),\n",
       " ('#iowa',\n",
       "  {'neg': 73.184,\n",
       "   'neu': 521.3799999999999,\n",
       "   'pos': 101.45099999999998,\n",
       "   'compound': 55.191799999999986}),\n",
       " ('#biggboss',\n",
       "  {'neg': 61.392999999999994,\n",
       "   'neu': 872.8369999999999,\n",
       "   'pos': 221.77900000000002,\n",
       "   'compound': 289.0983999999999}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 133.68300000000002,\n",
       "   'neu': 1076.398,\n",
       "   'pos': 147.925,\n",
       "   'compound': 23.1815})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "event_sentiments_l = aggreg.collect()\n",
    "t2 = time.time()\n",
    "time2 = t2-t1\n",
    "\n",
    "event_sentiments_l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentiment(x):\n",
    "    dict1    = x\n",
    "    sum_prob = dict1['neg'] + dict1['neu'] + dict1['pos']\n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] /= sum_prob\n",
    "    return dict1\n",
    "\n",
    "\n",
    "aggreg_normalized = aggreg.map(lambda x: (x[0], normalize_sentiment(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "event_sentiments_ln = aggreg_normalized.collect()\n",
    "t2 = time.time()\n",
    "time3 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 0.0040194870446218145,\n",
       "   'neu': 0.8361823639324092,\n",
       "   'pos': 0.15979814902296907,\n",
       "   'compound': 0.2838315484278386}),\n",
       " ('#arsd',\n",
       "  {'neg': 0.06625747677822555,\n",
       "   'neu': 0.8403805614939405,\n",
       "   'pos': 0.0933619617278341,\n",
       "   'compound': 0.06559285357805274}),\n",
       " ('#iowa',\n",
       "  {'neg': 0.1051471591847877,\n",
       "   'neu': 0.7490930511555067,\n",
       "   'pos': 0.1457597896597056,\n",
       "   'compound': 0.07929685423446334}),\n",
       " ('#biggboss',\n",
       "  {'neg': 0.05310771801949638,\n",
       "   'neu': 0.7550434295926761,\n",
       "   'pos': 0.19184885238782745,\n",
       "   'compound': 0.2500831740929352}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 0.09844065490137749,\n",
       "   'neu': 0.792631254942909,\n",
       "   'pos': 0.10892809015571361,\n",
       "   'compound': 0.0170702485850578}),\n",
       " ('#asimriaz',\n",
       "  {'neg': 0.06977859288060737,\n",
       "   'neu': 0.7629025185663331,\n",
       "   'pos': 0.16731888855305946,\n",
       "   'compound': 0.15989261982882397}),\n",
       " ('#curecancer',\n",
       "  {'neg': 0.2291470615972054,\n",
       "   'neu': 0.5569524341721126,\n",
       "   'pos': 0.213900504230682,\n",
       "   'compound': -0.09943524859395293}),\n",
       " ('#rashamidesai',\n",
       "  {'neg': 0.06565979381443297,\n",
       "   'neu': 0.7649621993127148,\n",
       "   'pos': 0.16937800687285218,\n",
       "   'compound': 0.17399931271477664})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_sentiments_ln[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments, aggregating and normalizing: 39.36s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments, aggregating and normalizing: {:.4g}s\".format(time1 + time2 + time3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing average sentiment on all tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_rdd  = words_f.select('filtered').rdd.map(lambda x: ' '.join(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sentences = sentences_rdd.collect()\n",
    "t2 = time.time()\n",
    "time4 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "overall_sent = {'neg': 0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "for sentence in sentences:\n",
    "    sentiment = sid.polarity_scores(sentence)    \n",
    "    for key in overall_sent.keys():\n",
    "        overall_sent[key] += sentiment[key]\n",
    "        \n",
    "    \n",
    "for key in overall_sent.keys():\n",
    "        overall_sent[key] /= num_sentences\n",
    "overall_sent\n",
    "t2 = time.time()\n",
    "time5 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments not using an rdd: 82.31s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments not using an rdd: {:.4g}s\".format(time4 + time5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "We can clearly see that using an rdd and utilizing the map reduce scheme gives us faster performance! Also, the naive algorithm above doesn't even group by hashtags..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
