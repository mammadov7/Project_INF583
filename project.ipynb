{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lower, col, size, length\n",
    "from operator import add\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession\\\n",
    "  .builder \\\n",
    "  .appName(\"Twitter_app\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre',\n",
       " 'http',\n",
       " 'https']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "## Reading from the stopword file:\n",
    "text_file = open(\"datasets/stop_words_english.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = text_file.read()\n",
    "\n",
    "## Creating the stop word array:\n",
    "stopWords = lines.split()\n",
    "# Adding http and https to it:\n",
    "stopWords.append('http')\n",
    "stopWords.append('https')\n",
    "stopWords[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = spark.read.format('json').options(header='true', inferSchema='true') \\\n",
    "  .load('./datasets/NoFilterEnglish2020-02-04.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------------------------------------------+\n|text                                                     |\n+---------------------------------------------------------+\n|@theythemsbian thank you for being brave enough to say it|\n+---------------------------------------------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "clean_texts = data.select('*', lower(f.regexp_replace(f.col('text'), r'[^a-zA-Z#@,!\\\\s]', ' ')).alias('text2'))\\\n",
    "                  .drop('text')\\\n",
    "                  .withColumnRenamed('text2', 'text')\n",
    "\n",
    "clean_texts.select('text').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------------------------------------------+\n|words_t                                                             |\n+--------------------------------------------------------------------+\n|[@theythemsbian, thank, you, for, being, brave, enough, to, say, it]|\n+--------------------------------------------------------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words_t')\n",
    "clean_word_tokens = tokenizer.transform(clean_texts)\n",
    "\n",
    "clean_word_tokens.select('words_t').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove useless words:\n",
    "\n",
    "By useless, we mean the words of size 2 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------------------------------------------+\n|words                                                               |\n+--------------------------------------------------------------------+\n|[@theythemsbian, thank, you, for, being, brave, enough, to, say, it]|\n+--------------------------------------------------------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "## Defining the function: (udf: user defined function)\n",
    "filter_length_udf = f.udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "#clean_word_tokens = clean_word_tokens.withColumn('words', filter_length_udf(col('words_t')))\n",
    "clean_word_tokens = clean_word_tokens.withColumn('words', col('words_t'))\n",
    "\n",
    "clean_word_tokens.select('words').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------+\n|                text|          word|\n+--------------------+--------------+\n|@theythemsbian th...|@theythemsbian|\n|@theythemsbian th...|         thank|\n|@theythemsbian th...|           you|\n|@theythemsbian th...|           for|\n+--------------------+--------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "exp_words = clean_word_tokens.withColumn('word', f.explode('words'))\\\n",
    "                             .drop('words')\n",
    "                 \n",
    "#Tokenizer\n",
    "exp_words.select('text', 'word').show(4)\n",
    "\n",
    "exp_words.createOrReplaceTempView(\"words\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreqWords(ts1, ts2, table = \"words\"):\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT word, count(*) AS word_count \" + \n",
    "                      \"FROM {} \".format(table) + \n",
    "                      \"WHERE {0}.timestamp_ms BETWEEN {1} AND {2} \".format(table, ts1, ts2)+ \n",
    "                      \"GROUP BY word \" + \n",
    "                      \"ORDER BY word_count DESC\")\n",
    "    sqlDF.show(10)\n",
    "    return sqlDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+\n| word|word_count|\n+-----+----------+\n|     |       121|\n|   rt|        21|\n|  the|        13|\n|    i|        10|\n|    t|         9|\n|   co|         7|\n|https|         7|\n|   is|         7|\n|   to|         7|\n| this|         6|\n+-----+----------+\nonly showing top 10 rows\n\ntime spent computing: 30.67\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-----+\n| word|count|\n+-----+-----+\n|     |  121|\n|   rt|   21|\n|  the|   13|\n|    i|   10|\n|    t|    9|\n|   to|    7|\n|   co|    7|\n|   is|    7|\n|https|    7|\n| this|    6|\n+-----+-----+\nonly showing top 10 rows\n\ntime spent computing: 29.23\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "## Timing the operation:\n",
    "t1 = time.time()\n",
    "\n",
    "exp_words.filter(f.col('timestamp_ms').between(ts1, ts2) )\\\n",
    "         .groupBy('word') \\\n",
    "         .count() \\\n",
    "         .sort('count', ascending=False) \\\n",
    "         .show(10)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "#         .filter(length(col(\"word\")) >= 3)\\\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "\n",
    "We can see that both methods are equivalent in computing time and yield to the same results. Choosing between both is just a question of taste. We Personally prefer **SQL** querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n|filtered                                                                                                           |\n+-------------------------------------------------------------------------------------------------------------------+\n|[@theythemsbian, brave]                                                                                            |\n|[rt, @camillediola, , duterte, violated, law, appointing, honasan,, background,, head, dict, , shrugged,, pointing]|\n|[rt, @tinyseokjinnie, , lol, bored, , , , dypgt, ti]                                                               |\n|[rt, @ibesuckafree, , suck, quitting, weed, ,, , , argument, , amp, , gettin, ]                                    |\n|[rt, @mikebloomberg, , donald, trump, bring, change, country, , , , , #superbowl, , , , vciycilow]                 |\n+-------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Removing the stopwords from the array of strings\n",
    "sc = spark.sparkContext\n",
    "broadcastVar = sc.broadcast(stopWords)\n",
    "broadcastVar.value\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=broadcastVar.value)\n",
    "words_f = remover.transform(clean_word_tokens)\n",
    "words_f = words_f.drop('words_t', 'words')\n",
    "\n",
    "words_f.select('filtered').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['truncated', 'user', 'withheld_in_countries', 'text', 'filtered']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words_filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------------------------------------------+--------------+\n|text                                                     |word          |\n+---------------------------------------------------------+--------------+\n|@theythemsbian thank you for being brave enough to say it|@theythemsbian|\n+---------------------------------------------------------+--------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "exp_words_f = words_f.withColumn('word', f.explode('filtered'))#\\\n",
    "                     #.drop('filtered')\n",
    "#.drop('filtered')???\n",
    "\n",
    "exp_words_f.select('text', 'word').show(1, False)\n",
    "\n",
    "exp_words_f.createOrReplaceTempView(\"words_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['user', 'withheld_in_countries', 'text', 'filtered', 'word']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "exp_words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time after filtering using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+----------+\n|         word|word_count|\n+-------------+----------+\n|             |       121|\n|           rt|        21|\n|         fuck|         2|\n|          amp|         2|\n|          pro|         2|\n|         rare|         2|\n|         rush|         2|\n|        rolls|         1|\n|     clinging|         1|\n|@chqmbiedolan|         1|\n+-------------+----------+\nonly showing top 10 rows\n\ntime spent computing: 27.35\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words_filtered\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+\n|  word|  count|\n+------+-------+\n|      |1061083|\n|    rt| 123377|\n|   don|   7913|\n|   amp|   7470|\n|     @|   7080|\n|people|   6434|\n|  love|   6162|\n|  time|   4881|\n|     ,|   4846|\n|   day|   4405|\n| trump|   4398|\n|  good|   4267|\n|     #|   3943|\n|    ve|   3685|\n|  iowa|   3213|\n|    ll|   3143|\n| today|   3118|\n|  shit|   3081|\n|   man|   2949|\n| happy|   2346|\n+------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "exp_words_f.filter(\"(timestamp_ms / 1000 / 60 / 60  % 24 )>= 20\") \\\n",
    "           .groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False) \\\n",
    "           .show()"
   ]
  },
  {
   "source": [
    "### Exercise 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_time = exp_words_f.select(\"word\", \"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o356.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 1791) (LAPTOP-G4N120DU executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:478)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor157.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:478)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 26 more\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-883e4a554e98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mwords_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mwords_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mwords_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"number\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"month\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hour\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o356.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 1791) (LAPTOP-G4N120DU executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:478)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor157.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, Le fichier spécifié est introuvable\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:478)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "#to format month in integer\n",
    "map_month = {month:index for index, month in enumerate(calendar.month_abbr) if month}\n",
    "\n",
    "udf_function = udf(lambda month: map_month[month], returnType=IntegerType())\n",
    "\n",
    "#Splitting the column created at\n",
    "split_col = f.split(exp_words_f['created_at'], \" |:\") \n",
    "\n",
    "words_time = words_time.withColumn('day', split_col.getItem(0))\n",
    "words_time = words_time.withColumn('month', udf_function(split_col.getItem(1))) #month represented with irs number\n",
    "words_time = words_time.withColumn('number', split_col.getItem(2))\n",
    "words_time = words_time.withColumn('hour', split_col.getItem(3))\n",
    "words_time = words_time.withColumn('min', split_col.getItem(4))\n",
    "words_time.select(\"word\", \"number\", \"month\", \"hour\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Compute the delta between two date formated as ['number', 'month', 'hour'] in hour\n",
    "def get_delta_hours(start, end):\n",
    "\n",
    "    s = datetime.datetime(year=2020, month=map_month[start[1]], day=int(start[0]), hour=int(start[2]), minute=0)\n",
    "    e = datetime.datetime(year=2020, month=map_month[end[1]], day=int(end[0]), hour=int(end[2]), minute=0)\n",
    "\n",
    "    time_delta = e-s\n",
    "    delta_hours = time_delta.days*24 + time_delta.seconds//3600\n",
    "\n",
    "    return delta_hours"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Poisson method. We consider that the count of a word is Poisson distributed \n",
    "\n",
    "def poisson_method(df, w, alpha, bucket_size):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    df : dataframe with columns word, month, day, hour\n",
    "    word : word to study\n",
    "    alpha : level of the confidence interval\n",
    "    bucket :size of the bucket to consider (in hour)\n",
    "\n",
    "    Returns a list of eta denoting the unlikeliness of a point\n",
    "    \"\"\"\n",
    "    df = df.filter(df.word==w)\n",
    "\n",
    "    #create a new dataframe with a column count\n",
    "    df = df.groupBy(\"word\", \"number\", \"month\", \"hour\").count()\n",
    "    df = df.orderBy(\"month\", \"number\", \"hour\")\n",
    "    df.show()\n",
    "\n",
    "    df_array = np.array(df.select(\"number\", \"month\", \"hour\",\"count\").collect())\n",
    "\n",
    "    delta_total = get_delta_hours(df_array[0], df_array[len(df_array)-1])\n",
    "\n",
    "    nb_buckets = delta_total//bucket_size\n",
    "    print(nb_buckets)\n",
    "    counts = [0 for i in range(nb_buckets+1)]\n",
    "\n",
    "    for i in range(len(df_array)):\n",
    "        delta_hour = get_delta_hours(df_array[0],df_array[i])\n",
    "        print(delta_hour//bucket_size)\n",
    "        counts[delta_hour//bucket_size]+=int(df_array[i][3])\n",
    "\n",
    "    unlikeliness = []\n",
    "    for i in range(1, len(counts)):\n",
    "        ci_low, ci_high = poisson.interval(alpha, counts[i-1]) \n",
    "        eta = abs(counts[i] -counts[i-1])/(ci_high+0.01) #add a small value to avoid division by zero\n",
    "        unlikeliness.append(eta)\n",
    "\n",
    "    plt.plot(np.arange(1, nb_buckets+1), np.array(unlikeliness), color='red', marker='o')\n",
    "    plt.plot(np.arange(1, nb_buckets+1), counts[1:], color='blue', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poisson correlated method. We use the data of the previous day at the same hour\n",
    "\n",
    "def poisson_correlated_method(df, w, alpha, bucket_size):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    df : dataframe with columns word, month, day, hour\n",
    "    word : word to study\n",
    "    alpha : level of the confidence interval\n",
    "    bucket :size of the bucket to consider (in hour)\n",
    "\n",
    "    Returns a list of eta denoting the unlikeliness of a point\n",
    "    \"\"\"\n",
    "    df = df.filter(df.word==w)\n",
    "\n",
    "    #create a new dataframe with a column count\n",
    "    df = df.groupBy(\"word\", \"number\", \"month\", \"hour\").count()\n",
    "    df = df.orderBy(\"month\", \"number\", \"hour\")\n",
    "    df.show(5)\n",
    "\n",
    "    df_array = np.array(df.select(\"number\", \"month\", \"hour\",\"count\").collect())\n",
    "\n",
    "    delta_total = get_delta_hours(df_array[0], df_array[len(df_array)-1])\n",
    "\n",
    "    nb_buckets = delta_total//bucket_size\n",
    "    counts = [0 for i in range(nb_buckets+1)]\n",
    "\n",
    "    for i in range(len(df_array)):\n",
    "        delta_hour = get_delta_hours(df_array[0],df_array[i])\n",
    "        counts[delta_hour//bucket_size]+=int(df_array[i][3])\n",
    "\n",
    "    unlikeliness = []\n",
    "    for i in range(24//bucket_size, len(counts)):\n",
    "        ci_low, ci_high = poisson.interval(alpha, counts[i-24//bucket_size]) #remove 24//bucket_size to obtain the corresponding bucket of the previous day\n",
    "        eta = abs(counts[i] -counts[i-1])/(ci_high+0.01) #add a small value to avoid division by zero\n",
    "        unlikeliness.append(eta)\n",
    "\n",
    "    plt.plot(np.arange(24//bucket_size, nb_buckets+1), np.array(unlikeliness), color='red', marker='o')\n",
    "    plt.plot(np.arange(24//bucket_size, nb_buckets+1), counts[1:], color='blue', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+------+-----+----+\n",
      "|          word|number|month|hour|\n",
      "+--------------+------+-----+----+\n",
      "|@theythemsbian|    03|  Feb|  22|\n",
      "|         brave|    03|  Feb|  22|\n",
      "|            rt|    03|  Feb|  22|\n",
      "| @camillediola|    03|  Feb|  22|\n",
      "|              |    03|  Feb|  22|\n",
      "+--------------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+------+-----+----+-----+\n",
      "|word|number|month|hour|count|\n",
      "+----+------+-----+----+-----+\n",
      "|test|    03|  Feb|  23|   41|\n",
      "|test|    04|  Feb|  00|   43|\n",
      "|test|    04|  Feb|  01|   33|\n",
      "|test|    04|  Feb|  02|   49|\n",
      "|test|    04|  Feb|  03|   49|\n",
      "|test|    04|  Feb|  04|   53|\n",
      "|test|    04|  Feb|  05|   47|\n",
      "|test|    04|  Feb|  06|   44|\n",
      "|test|    04|  Feb|  07|   42|\n",
      "|test|    04|  Feb|  08|   32|\n",
      "|test|    04|  Feb|  09|   40|\n",
      "|test|    04|  Feb|  10|   38|\n",
      "|test|    04|  Feb|  11|    7|\n",
      "|test|    04|  Feb|  12|   29|\n",
      "|test|    04|  Feb|  13|   25|\n",
      "|test|    04|  Feb|  14|   23|\n",
      "|test|    04|  Feb|  15|   32|\n",
      "|test|    04|  Feb|  16|   68|\n",
      "|test|    04|  Feb|  17|   66|\n",
      "|test|    04|  Feb|  18|   71|\n",
      "+----+------+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "23\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 368.925 248.518125\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-03-26T13:54:21.619360</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 368.925 248.518125 \r\nL 368.925 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\nL 361.725 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m237eb5c2c6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.308471\" xlink:href=\"#m237eb5c2c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(25.127221 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.482025\" xlink:href=\"#m237eb5c2c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(94.300775 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.655579\" xlink:href=\"#m237eb5c2c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(160.293079 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.829132\" xlink:href=\"#m237eb5c2c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(229.466632 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"305.002686\" xlink:href=\"#m237eb5c2c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(298.640186 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0c75341300\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 218.555582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"186.915134\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 190.714353)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"159.073905\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 162.873124)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"131.232676\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 135.031895)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"103.391447\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 107.190666)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"75.550218\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(7.2 79.349436)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"47.708988\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(7.2 51.508207)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0c75341300\" y=\"19.867759\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 70 -->\r\n      <g transform=\"translate(7.2 23.666978)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p3083647327)\" d=\"M 42.143182 214.653267 \r\nL 55.977893 214.259288 \r\nL 69.812603 213.766673 \r\nL 83.647314 214.756364 \r\nL 97.482025 214.579622 \r\nL 111.316736 214.510742 \r\nL 125.151446 214.619462 \r\nL 138.986157 214.658692 \r\nL 152.820868 214.250251 \r\nL 166.655579 214.250274 \r\nL 180.490289 214.651322 \r\nL 194.325 213.064385 \r\nL 208.159711 210.048393 \r\nL 221.994421 214.478021 \r\nL 235.829132 214.597316 \r\nL 249.663843 213.997287 \r\nL 263.498554 212.478962 \r\nL 277.333264 214.690863 \r\nL 291.167975 214.586621 \r\nL 305.002686 214.598193 \r\nL 318.837397 214.722415 \r\nL 332.672107 214.353888 \r\nL 346.506818 214.676829 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m1f5e33e1d7\" style=\"stroke:#ff0000;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p3083647327)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"42.143182\" xlink:href=\"#m1f5e33e1d7\" y=\"214.653267\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"55.977893\" xlink:href=\"#m1f5e33e1d7\" y=\"214.259288\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"69.812603\" xlink:href=\"#m1f5e33e1d7\" y=\"213.766673\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"83.647314\" xlink:href=\"#m1f5e33e1d7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"97.482025\" xlink:href=\"#m1f5e33e1d7\" y=\"214.579622\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"111.316736\" xlink:href=\"#m1f5e33e1d7\" y=\"214.510742\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"125.151446\" xlink:href=\"#m1f5e33e1d7\" y=\"214.619462\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"138.986157\" xlink:href=\"#m1f5e33e1d7\" y=\"214.658692\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"152.820868\" xlink:href=\"#m1f5e33e1d7\" y=\"214.250251\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"166.655579\" xlink:href=\"#m1f5e33e1d7\" y=\"214.250274\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"180.490289\" xlink:href=\"#m1f5e33e1d7\" y=\"214.651322\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"194.325\" xlink:href=\"#m1f5e33e1d7\" y=\"213.064385\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"208.159711\" xlink:href=\"#m1f5e33e1d7\" y=\"210.048393\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"221.994421\" xlink:href=\"#m1f5e33e1d7\" y=\"214.478021\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"235.829132\" xlink:href=\"#m1f5e33e1d7\" y=\"214.597316\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"249.663843\" xlink:href=\"#m1f5e33e1d7\" y=\"213.997287\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"263.498554\" xlink:href=\"#m1f5e33e1d7\" y=\"212.478962\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"277.333264\" xlink:href=\"#m1f5e33e1d7\" y=\"214.690863\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"291.167975\" xlink:href=\"#m1f5e33e1d7\" y=\"214.586621\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"305.002686\" xlink:href=\"#m1f5e33e1d7\" y=\"214.598193\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"318.837397\" xlink:href=\"#m1f5e33e1d7\" y=\"214.722415\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"332.672107\" xlink:href=\"#m1f5e33e1d7\" y=\"214.353888\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"346.506818\" xlink:href=\"#m1f5e33e1d7\" y=\"214.676829\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p3083647327)\" d=\"M 42.143182 95.039078 \r\nL 55.977893 122.880307 \r\nL 69.812603 78.334341 \r\nL 83.647314 78.334341 \r\nL 97.482025 67.197849 \r\nL 111.316736 83.902586 \r\nL 125.151446 92.254955 \r\nL 138.986157 97.823201 \r\nL 152.820868 125.66443 \r\nL 166.655579 103.391447 \r\nL 180.490289 108.959693 \r\nL 194.325 195.267503 \r\nL 208.159711 134.016799 \r\nL 221.994421 145.153291 \r\nL 235.829132 150.721536 \r\nL 249.663843 125.66443 \r\nL 263.498554 25.436005 \r\nL 277.333264 31.004251 \r\nL 291.167975 17.083636 \r\nL 305.002686 31.004251 \r\nL 318.837397 28.220128 \r\nL 332.672107 61.629603 \r\nL 346.506818 56.061357 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m3d58e0aa41\" style=\"stroke:#0000ff;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p3083647327)\">\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"42.143182\" xlink:href=\"#m3d58e0aa41\" y=\"95.039078\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"55.977893\" xlink:href=\"#m3d58e0aa41\" y=\"122.880307\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"69.812603\" xlink:href=\"#m3d58e0aa41\" y=\"78.334341\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"83.647314\" xlink:href=\"#m3d58e0aa41\" y=\"78.334341\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"97.482025\" xlink:href=\"#m3d58e0aa41\" y=\"67.197849\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"111.316736\" xlink:href=\"#m3d58e0aa41\" y=\"83.902586\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"125.151446\" xlink:href=\"#m3d58e0aa41\" y=\"92.254955\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"138.986157\" xlink:href=\"#m3d58e0aa41\" y=\"97.823201\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"152.820868\" xlink:href=\"#m3d58e0aa41\" y=\"125.66443\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"166.655579\" xlink:href=\"#m3d58e0aa41\" y=\"103.391447\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"180.490289\" xlink:href=\"#m3d58e0aa41\" y=\"108.959693\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"194.325\" xlink:href=\"#m3d58e0aa41\" y=\"195.267503\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"208.159711\" xlink:href=\"#m3d58e0aa41\" y=\"134.016799\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"221.994421\" xlink:href=\"#m3d58e0aa41\" y=\"145.153291\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"235.829132\" xlink:href=\"#m3d58e0aa41\" y=\"150.721536\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"249.663843\" xlink:href=\"#m3d58e0aa41\" y=\"125.66443\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"263.498554\" xlink:href=\"#m3d58e0aa41\" y=\"25.436005\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"277.333264\" xlink:href=\"#m3d58e0aa41\" y=\"31.004251\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"291.167975\" xlink:href=\"#m3d58e0aa41\" y=\"17.083636\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"305.002686\" xlink:href=\"#m3d58e0aa41\" y=\"31.004251\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"318.837397\" xlink:href=\"#m3d58e0aa41\" y=\"28.220128\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"332.672107\" xlink:href=\"#m3d58e0aa41\" y=\"61.629603\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"346.506818\" xlink:href=\"#m3d58e0aa41\" y=\"56.061357\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 361.725 224.64 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p3083647327\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "data_by_hour = words_time.select(\"word\", \"number\", \"month\", \"hour\")\n",
    "data_by_hour.show(5)\n",
    "\n",
    "#Apply poisson method with hourly buckets\n",
    "eta = poisson_method(data_by_hour, 'word', 0.95, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = exp_words_f.filter( exp_words_f.word.startswith('#'))\\\n",
    "                      .withColumnRenamed('word', 'hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             hashtag|count|\n",
      "+--------------------+-----+\n",
      "|         #curecancer| 3987|\n",
      "|       #iowacaucuses| 3253|\n",
      "|#deshkiawaazshehnaaz| 2871|\n",
      "|    #mainbhiasimkigf| 2782|\n",
      "|      #meraadarshsid| 2685|\n",
      "|       #iheartawards| 1753|\n",
      "|                #bts| 1658|\n",
      "|           #asimriaz| 1512|\n",
      "|        #thebachelor| 1358|\n",
      "|         #iowacaucus| 1274|\n",
      "|                 #bb| 1251|\n",
      "|     #sidharthshukla| 1245|\n",
      "|           #treasure| 1232|\n",
      "|        #coronavirus| 1197|\n",
      "|           #iacaucus| 1197|\n",
      "|           #biggboss| 1156|\n",
      "|         #loveisland|  999|\n",
      "|          #superbowl|  976|\n",
      "|     #worldcancerday|  965|\n",
      "|               #sotu|  894|\n",
      "|     #rightchoicesid|  843|\n",
      "|              #yikes|  776|\n",
      "|     #happyjisungday|  710|\n",
      "|               #iowa|  696|\n",
      "|            #btsarmy|  686|\n",
      "|         #nowplaying|  646|\n",
      "|                #nct|  640|\n",
      "|       #rashamidesai|  582|\n",
      "|                #raw|  581|\n",
      "|             #superm|  574|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_freq_hash = hashtags.groupBy('hashtag') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False) \n",
    "most_freq_hash.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting Hashtag Data Frame to array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfht = most_freq_hash.filter('count >= 500').select('hashtag').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting words related to Hashtag example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#iowacaucuses',\n",
       " 'iowa',\n",
       " 'bernie',\n",
       " 'caucus',\n",
       " 'vote',\n",
       " 'won',\n",
       " 'tonight',\n",
       " '#iowacaucus',\n",
       " 'biden',\n",
       " 'app',\n",
       " '@donaldjtrumpjr',\n",
       " 'dnc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_words = exp_words_f.filter( f.col('text').contains('#iowacaucuses'))\n",
    "\n",
    "event_words.groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False)\\\n",
    "           .rdd\\\n",
    "           .map(lambda x: x[0])\\\n",
    "           .collect()[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex5 using pySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+\n",
      "|hashtag|       word|count|\n",
      "+-------+-----------+-----+\n",
      "| #yikes|@nickiminaj|  465|\n",
      "| #yikes|      likes|  348|\n",
      "| #yikes|        amp|  336|\n",
      "| #yikes|       play|  325|\n",
      "| #yikes|       life|  313|\n",
      "| #yikes|        tag|  313|\n",
      "| #yikes|    yzdifvf|  310|\n",
      "| #yikes|       rosa|  137|\n",
      "| #yikes|      parks|  129|\n",
      "| #yikes|      nicki|  121|\n",
      "+-------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ex.5: Get the most frequent words for each event:\n",
    "ht_f = hashtags.select('filtered', 'hashtag')\\\n",
    "                         .filter(f.col('hashtag').isin(mfht))\\\n",
    "                         .withColumn('word', f.explode('filtered'))\\\n",
    "                         .filter(~col('word').startswith('#'))\\\n",
    "                         .select('hashtag', 'word')\\\n",
    "                         .groupBy('hashtag', 'word')\\\n",
    "                         .count()\\\n",
    "                         .sort(['hashtag', 'count'], ascending=False)\\\n",
    "                         .filter('count >= 100')\n",
    "\n",
    "ht_f.show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the top 10 most frequent words by events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|    hashtag|       word|count|\n",
      "+-----------+-----------+-----+\n",
      "|#curecancer|trueworship| 3951|\n",
      "|#curecancer|     cancer| 2441|\n",
      "|#curecancer|       true| 1240|\n",
      "|#curecancer|     rampal| 1073|\n",
      "|#curecancer|    worship| 1042|\n",
      "|#curecancer|      saint| 1040|\n",
      "|#curecancer|       cure|  916|\n",
      "|#curecancer|        god|  884|\n",
      "|#curecancer|    maharaj|  859|\n",
      "|#curecancer|   diseases|  846|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+---------------+-----+\n",
      "|      hashtag|           word|count|\n",
      "+-------------+---------------+-----+\n",
      "|#iowacaucuses|           iowa|  499|\n",
      "|#iowacaucuses|         bernie|  346|\n",
      "|#iowacaucuses|         caucus|  237|\n",
      "|#iowacaucuses|           vote|  193|\n",
      "|#iowacaucuses|            won|  173|\n",
      "|#iowacaucuses|        tonight|  155|\n",
      "|#iowacaucuses|          biden|  153|\n",
      "|#iowacaucuses|            app|  151|\n",
      "|#iowacaucuses|@donaldjtrumpjr|  150|\n",
      "|#iowacaucuses|            dnc|  147|\n",
      "+-------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+-----+\n",
      "|             hashtag|     word|count|\n",
      "+--------------------+---------+-----+\n",
      "|#deshkiawaazshehnaaz|     sana|  459|\n",
      "|#deshkiawaazshehnaaz|     love|  287|\n",
      "|#deshkiawaazshehnaaz|     guys|  235|\n",
      "|#deshkiawaazshehnaaz| shehnaaz|  193|\n",
      "|#deshkiawaazshehnaaz|    trend|  143|\n",
      "|#deshkiawaazshehnaaz|@colorstv|  141|\n",
      "|#deshkiawaazshehnaaz|    speed|  128|\n",
      "|#deshkiawaazshehnaaz| trending|  119|\n",
      "|#deshkiawaazshehnaaz|   follow|  116|\n",
      "|#deshkiawaazshehnaaz|      hai|  113|\n",
      "+--------------------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-----------+-----+\n",
      "|         hashtag|       word|count|\n",
      "+----------------+-----------+-----+\n",
      "|#mainbhiasimkigf|       asim|  982|\n",
      "|#mainbhiasimkigf|      trend|  485|\n",
      "|#mainbhiasimkigf|      girls|  482|\n",
      "|#mainbhiasimkigf|   trending|  291|\n",
      "|#mainbhiasimkigf|  @colorstv|  253|\n",
      "|#mainbhiasimkigf|       love|  214|\n",
      "|#mainbhiasimkigf|   fangirls|  202|\n",
      "|#mainbhiasimkigf|        fan|  193|\n",
      "|#mainbhiasimkigf|  @biggboss|  187|\n",
      "|#mainbhiasimkigf|@imrealasim|  186|\n",
      "+----------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+---------------+-----+\n",
      "|       hashtag|           word|count|\n",
      "+--------------+---------------+-----+\n",
      "|#meraadarshsid|            sid|  516|\n",
      "|#meraadarshsid|         shukla|  428|\n",
      "|#meraadarshsid|      @sidharth|  352|\n",
      "|#meraadarshsid|         tweets|  282|\n",
      "|#meraadarshsid|@realvindusingh|  281|\n",
      "|#meraadarshsid|          today|  228|\n",
      "|#meraadarshsid|            tag|  223|\n",
      "|#meraadarshsid|      @biggboss|  180|\n",
      "|#meraadarshsid|       activity|  162|\n",
      "|#meraadarshsid|        tagline|  160|\n",
      "+--------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+----------------+-----+\n",
      "|      hashtag|            word|count|\n",
      "+-------------+----------------+-----+\n",
      "|#iheartawards|            @bts|  567|\n",
      "|#iheartawards|             twt|  553|\n",
      "|#iheartawards|            vote|  536|\n",
      "|#iheartawards|             sos|  264|\n",
      "|#iheartawards|   @justinbieber|  259|\n",
      "|#iheartawards|     @danandshay|  226|\n",
      "|#iheartawards|            @tha|  220|\n",
      "|#iheartawards|@ygofficialblink|  216|\n",
      "|#iheartawards|          voting|  200|\n",
      "|#iheartawards|           hours|  187|\n",
      "+-------------+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    ht_f.filter(col('hashtag').isin(ht)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that converts geolocalisation to place name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(coords):\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")  \n",
    "    if coords.shape != (2,):\n",
    "        coords = coords.mean(axis = 0)\n",
    "    \n",
    "    Longitude = str(coords[0])\n",
    "    Latitude = str(coords[1])\n",
    "    \n",
    "    location = geolocator.reverse(Latitude+\",\"+Longitude)\n",
    "    \n",
    "    if(location == None):\n",
    "        print(\"No location specified\")\n",
    "        return\n",
    " \n",
    "    address = location.raw['address']\n",
    "\n",
    "    # traverse the data\n",
    "    city = address.get('city', '')\n",
    "    state = address.get('state', '')\n",
    "    country = address.get('country', '')\n",
    "    code = address.get('country_code')\n",
    "    zipcode = address.get('postcode')\n",
    "    print('City : ', city)\n",
    "    print('State : ', state)\n",
    "    print('Country : ', country)\n",
    "    print('Zip Code : ', zipcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 and Exercice 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City :  \n",
      "State :  Madhya Pradesh\n",
      "Country :  India\n",
      "Zip Code :  476332\n",
      "Event : #curecancer\n",
      "Timeframe : from - 2020-02-04 03:16:13 to - 2020-02-04 22:42:39\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#curecancer', 'trueworship', 'cancer', 'true', 'rampal', 'saint', 'worship', 'cure', 'god', 'maharaj', 'diseases', 'disease']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  West Virginia\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #iowacaucuses\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:58:28\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucuses', 'iowa', 'bernie', 'caucus', 'vote', 'won', 'tonight', '#iowacaucus', 'biden', 'app', '@donaldjtrumpjr', 'dnc']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Maharashtra\n",
      "Country :  India\n",
      "Zip Code :  431201\n",
      "Event : #deshkiawaazshehnaaz\n",
      "Timeframe : from - 2020-02-04 01:58:49 to - 2020-02-04 22:59:12\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#deshkiawaazshehnaaz', 'sana', 'love', 'guys', 'shehnaaz', 'trend', '@colorstv', 'speed', 'trending', 'follow', 'hai', 'tag']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #mainbhiasimkigf\n",
      "Timeframe : from - 2020-02-03 23:26:18 to - 2020-02-04 22:58:06\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#mainbhiasimkigf', 'asim', 'trend', 'girls', 'trending', '#asimriaz', '@colorstv', 'love', 'fangirls', 'fan', '@imrealasim', '@biggboss']\n",
      "\n",
      "\n",
      "City :  Mumbai\n",
      "State :  Maharashtra\n",
      "Country :  India\n",
      "Zip Code :  400065\n",
      "Event : #meraadarshsid\n",
      "Timeframe : from - 2020-02-04 08:30:32 to - 2020-02-04 22:59:16\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#meraadarshsid', 'sid', 'shukla', '@sidharth', '@realvindusingh', 'tweets', 'today', 'tag', '@biggboss', 'activity', 'tagline', '@herdhush']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #iheartawards\n",
      "Timeframe : from - 2020-02-03 23:00:38 to - 2020-02-04 22:58:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iheartawards', '@bts', 'twt', 'vote', '#bestfanarmy', '#bestmusicvideo', '#bestlyrics', '#bestcoversong', '#btsarmy', 'sos', '@justinbieber', '#boywithluv']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  North Carolina\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #bts\n",
      "Timeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:59:54\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#bts', '@bts', 'twt', '#btsarmy', 'bts', 'soul', 'outro', 'comeback', 'trailer', '#bestfanarmy', '#btsxzachsang', '@bighitent']\n",
      "\n",
      "\n",
      "City :  Mumbai\n",
      "State :  Maharashtra\n",
      "Country :  India\n",
      "Zip Code :  400065\n",
      "Event : #asimriaz\n",
      "Timeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:59:07\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#asimriaz', 'asim', '#mainbhiasimkigf', 'fans', 'amp', '#sidharthshukla', '#biggboss', '#bb', '#asimfandomhits', '#rashamidesai', 'love', '@biggboss']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kentucky\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #thebachelor\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:54:09\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#thebachelor', 'peter', 'kelsey', 'kelley', 'tammy', 'mykenna', '#thebachelorabc', 'drama', 'bachelor', 'girls', 'season', 'rose']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Ohio\n",
      "Country :  United States\n",
      "Zip Code :  45662\n",
      "Event : #iowacaucus\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucuses', '#iowacaucus', 'iowa', '#iowacaucusdisaster', 'bernie', 'caucus', 'app', 'breaking', 'party', 'vote', 'democrats', 'dnc']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kogi\n",
      "Country :  Nigeria\n",
      "Zip Code :  None\n",
      "Event : #bb\n",
      "Timeframe : from - 2020-02-03 23:05:20 to - 2020-02-04 22:58:37\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#bb', '#biggboss', '#bbb', 'shukla', '#sidharthshukla', 'prior', 'pyong', '@cleytu', 'tweet', 'appreciation', 'thnbpfwynp', 'jantando']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #sidharthshukla\n",
      "Timeframe : from - 2020-02-03 23:15:22 to - 2020-02-04 22:56:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#sidharthshukla', 'amp', '#bb', 'shukla', '#biggboss', '@sidharth', '#asimriaz', 'sid', '#meraadarshsid', 'hai', '#rightchoicesid', '#shehnaazgill']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #treasure\n",
      "Timeframe : from - 2020-02-03 23:01:44 to - 2020-02-04 22:47:25\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#treasure', 'editorial', 'treasure', 'week', 'maker', 'photography', 'official', '@ygent', '#vol', 'bang', 'dam', 'asahi']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Missouri\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #iacaucus\n",
      "Timeframe : from - 2020-02-03 23:04:16 to - 2020-02-04 22:59:04\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iacaucus', 'iowa', '#iowacaucuses', '@andrewgillum', '@gopchairwoman', 'wait', 'precincts', 'delays', 'hear', 'bad', 'winner', 'black']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #coronavirus\n",
      "Timeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:59:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#coronavirus', 'china', 'evil', '#coronavirusoutbreak', '#china', 'cases', 'chinese', '#coronaviruschina', 'confirmed', 'medical', 'hospital', 'patients']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Telangana\n",
      "Country :  India\n",
      "Zip Code :  500089\n",
      "Event : #biggboss\n",
      "Timeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:56:29\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#biggboss', '#bb', '#sidharthshukla', 'asim', '@colorstv', 'amp', 'fans', 'riaz', 'boss', '#asimriaz', 'bigg', 'shukla']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #loveisland\n",
      "Timeframe : from - 2020-02-03 23:00:30 to - 2020-02-04 22:59:25\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#loveisland', 'callum', 'shaughna', 'casa', 'mike', 'amor', 'ched', 'villa', 'girl', 'luke', 'boy', 'molly']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #superbowl\n",
      "Timeframe : from - 2020-02-03 22:59:55 to - 2020-02-04 22:52:53\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#superbowl', 'trump', '#superbowlliv', 'anthem', 'national', 'halftime', 'mocking', '@shakira', 'stand', 'caught', 'gop', 'takes']\n",
      "\n",
      "\n",
      "City :  Sidi Ghanem سيدي غانم\n",
      "State :  \n",
      "Country :  Maroc / ⵍⵎⵖⵔⵉⴱ / المغرب\n",
      "Zip Code :  None\n",
      "Event : #worldcancerday\n",
      "Timeframe : from - 2020-02-03 23:29:10 to - 2020-02-04 22:58:40\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#worldcancerday', 'cancer', 'today', 'disease', 'day', 'amp', 'people', '#cancer', 'awareness', 'support', 'terrible', 'battling']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #sotu\n",
      "Timeframe : from - 2020-02-03 23:08:18 to - 2020-02-04 22:59:43\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#sotu', 'tomorrow', 'trump', 'president', 'tonight', 'attend', 'join', 'people', '@potus', '@flotus', 'capitol', 'inspiring']\n",
      "\n",
      "\n",
      "City :  Karnal\n",
      "State :  Haryana\n",
      "Country :  India\n",
      "Zip Code :  132001\n",
      "Event : #rightchoicesid\n",
      "Timeframe : from - 2020-02-03 23:00:39 to - 2020-02-04 21:47:09\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#rightchoicesid', 'sid', 'shukla', '@sidharth', '@realvindusingh', '#sidharthshukla', 'guys', '#bb', 'trend', 'win', '@biggboss', '@colorstv']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kansas\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #yikes\n",
      "Timeframe : from - 2020-02-04 03:15:25 to - 2020-02-04 22:57:35\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#yikes', '@nickiminaj', '#yikesnm', 'likes', 'amp', 'play', 'life', '#it', 'tag', 'yzdifvf', 'nicki', 'rosa']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #happyjisungday\n",
      "Timeframe : from - 2020-02-04 03:27:07 to - 2020-02-04 22:59:53\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#happyjisungday', 'birthday', 'happy', '@nctsmtown', '#nct', '#nctdream', '#jisung', 'slthg', 'jisung', 'park', 'baby', 'dream']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Kentucky\n",
      "Country :  United States\n",
      "Zip Code :  41006\n",
      "Event : #iowa\n",
      "Timeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucuses', '#iowacaucus', 'iowa', '#iowa', '#iowacaucusdisaster', 'bernie', 'caucus', '#iowacaucas', 'app', 'democrats', 'tonight', 'party']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Wisconsin\n",
      "Country :  United States\n",
      "Zip Code :  54241\n",
      "Event : #btsarmy\n",
      "Timeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:58:56\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#btsarmy', '@bts', 'twt', '#bestfanarmy', '#iheartawards', 'message', 'bts', '@choi', 'album', 'era', '@btsvotingteam', 'army']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #nowplaying\n",
      "Timeframe : from - 2020-02-03 23:00:17 to - 2020-02-04 22:56:54\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#nowplaying', 'listen', 'radio', 'live', 'amp', 'tune', 'love', 'feat', '#listenlive', 'music', 'mix', '#radio']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #nct\n",
      "Timeframe : from - 2020-02-03 23:09:25 to - 2020-02-04 22:59:53\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#nct', '#nctdream', '#happyjisungday', '#jisung', 'birthday', 'happy', '@nctsmtown', 'slthg', '#superm', 'dream', '#taeyong', 'icn']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #rashamidesai\n",
      "Timeframe : from - 2020-02-03 23:03:45 to - 2020-02-04 22:58:18\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#rashamidesai', '#asimriaz', 'amp', '#sidharthshukla', '#invinciblerashamidesai', 'khabri', '@biggboss', '@real', '#bb', 'breaking', '@therashamidesai', 'elite']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Missouri\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #raw\n",
      "Timeframe : from - 2020-02-03 23:07:33 to - 2020-02-04 22:56:17\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#raw', '@wwe', 'wwe', '@wweuniverse', '#wwe', '@wweasuka', '@rubyriottwwe', '@randyorton', 'tonight', 'rematch', '@angelgarzawwe', 'challenge']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  North Carolina\n",
      "Country :  United States\n",
      "Zip Code :  27239:27292\n",
      "Event : #superm\n",
      "Timeframe : from - 2020-02-03 23:01:58 to - 2020-02-04 22:58:32\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#superm', 'live', '@superm', 'superm', 'san', 'jose', 'future', '#superminsanjose', '#superminsj', '#wearethefuture', '#supermthefuture', '#ten']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #sb\n",
      "Timeframe : from - 2020-02-03 23:01:37 to - 2020-02-04 22:58:54\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#sb', '@sb', 'official', 'billboardmainstay', '#wishbustuessb', '#sbliv', '#morhot', '@mor', 'alab', 'onawesamsungcon', '@nfl', '@jlo']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Tennessee\n",
      "Country :  United States\n",
      "Zip Code :  38482\n",
      "Event : #iowacaucusdisaster\n",
      "Timeframe : from - 2020-02-04 04:46:17 to - 2020-02-04 22:59:52\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#iowacaucusdisaster', 'party', 'democrats', 'iowa', 'cnn', 'dnc', 'crying', 'died', 'betrayed', 'terrence', 'caucus', '@carpedonktum']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #arsd\n",
      "Timeframe : from - 2020-02-03 23:01:05 to - 2020-02-04 22:54:08\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#arsd', '#armyselcaday', '@bts', 'twt', 'drop', '#armyseicaday', 'hype', 'arsd', '#bts', 'love', '#btsarmy', '@drwnk']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  Colorado\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #yanggang\n",
      "Timeframe : from - 2020-02-03 23:01:02 to - 2020-02-04 22:58:14\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#yanggang', '@andrewyang', '#caucusforyang', 'precinct', 'iowa', 'yang', 'amp', '#iowacaucuses', 'delegates', 'caucus', 'delegate', '@indiana']\n",
      "\n",
      "\n",
      "City :  Manchester\n",
      "State :  England\n",
      "Country :  United Kingdom\n",
      "Zip Code :  M14 4DL\n",
      "Event : #giveaway\n",
      "Timeframe : from - 2020-02-03 23:00:24 to - 2020-02-04 22:54:04\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#giveaway', 'follow', '@originalfunko', 'amp', 'win', 'chance', 'retweet', 'pop', '#funko', '#funkopop', 'enter', 'giveaway']\n",
      "\n",
      "\n",
      "City :  \n",
      "State :  North Carolina\n",
      "Country :  United States\n",
      "Zip Code :  None\n",
      "Event : #trump\n",
      "Timeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:58:58\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#trump', '#trumpsguilty', 'trump', '#kag', '@realdonaldtrump', '#maga', '@funder', 'landslide', '#trumpisanidiot', '#iowacaucus', '@joncoopertweets', 'hand']\n",
      "\n",
      "\n",
      "No location specified\n",
      "Event : #shehnaazgill\n",
      "Timeframe : from - 2020-02-03 23:02:57 to - 2020-02-04 22:55:40\n",
      "Most frequent Location is : Mumbai, India\n",
      "Most frequent words associated to the event: \n",
      "['#shehnaazgill', '#sidharthshukla', '#sidnaaz', '@colorstv', '#deshkiawaazshehnaaz', '@beingsalmankhan', 'shukla', 'india', 'hai', '@vivo', 'vote', '#bb']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    event_words = exp_words_f.filter( f.col('text').contains(ht))\n",
    "    event_texts = clean_texts.filter( f.col('text').contains(ht))\n",
    "\n",
    "    ## Ex.5: Get the most frequent words for each event:\n",
    "    most_freq_w = event_words.groupBy('word') \\\n",
    "                             .count() \\\n",
    "                             .sort('count', ascending=False)\\\n",
    "                             .rdd\\\n",
    "                             .map(lambda x: x[0])\\\n",
    "                             .collect()[:12]    \n",
    "    \n",
    "    # Ex.6 : Finding timeframe of the Event\n",
    "    end   = event_texts.select(f.max(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    start = event_texts.select(f.min(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    \n",
    "    # Ex.7 : Finding location of the Event\n",
    "    coords = event_texts.na.drop(subset=[\"place\"]) \\\n",
    "                        .select(\"place.bounding_box.coordinates\") \\\n",
    "                        .rdd.map(list).collect()\n",
    "    \n",
    "    ## Gets the most common location from the tweets !\n",
    "    location_name = event_texts_na.groupBy('place.name', 'place.country') \\\n",
    "                                  .count()\\\n",
    "                                  .sort('count', ascending=False)\\\n",
    "                                  .rdd\\\n",
    "                                  .map(lambda x : x[0] + ', ' + x[1])\\\n",
    "                                  .collect()\n",
    "    \n",
    "\n",
    "    if(coords != []):\n",
    "        coords_mean = np.array(coords).squeeze().mean(axis = 0)\n",
    "        loc(coords_mean)\n",
    "    else: \n",
    "        print(\"No location specified\")\n",
    "    \n",
    "    if(most_freq_w == []):\n",
    "        most_freq_w = 'no words found...'\n",
    "    \n",
    "    if(location_name != []):\n",
    "        location_name = location_name[0]\n",
    "    else:\n",
    "        location_name = 'No Location specified'\n",
    "    \n",
    "\n",
    "    end = str( pd.to_datetime(end, unit='ms').to_pydatetime())\n",
    "    start = str(pd.to_datetime(start, unit='ms').to_pydatetime())\n",
    "    print(\"Event : \" + ht)\n",
    "    print(\"Timeframe : from - \" + start[:-7] + \" to - \" + end[:-7])\n",
    "    print(\"Most frequent Location is : \" + location_name)\n",
    "    print(\"Most frequent words associated to the event: \\n\" + str(most_freq_w) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "time spent computing: 6.778\n",
      "Method 2\n",
      "time spent computing: 6.634\n",
      "Method 3\n",
      "time spent computing: 6.558\n",
      "Method 4\n",
      "time spent computing: 0.009586\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Use describe()\n",
    "t1 = time.time()\n",
    "float(event_words.describe(\"timestamp_ms\").filter(\"summary = 'max'\").select(\"timestamp_ms\").collect()[0].asDict()['timestamp_ms'])\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 1\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 2: Use SQL\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.registerTempTable(\"df_table\")\n",
    "spark.sql(\"SELECT MAX(timestamp_ms) as maxval FROM df_table\").collect()[0].asDict()['maxval']\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 2\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 3: Convert to RDD\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(\"timestamp_ms\").rdd.max()[0]\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 3\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "\n",
    "# Method 4: select\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(f.max(f.col('timestamp_ms')))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 4\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "\n",
    "Use NLP package from [nltk](https://www.nltk.org/api/nltk.sentiment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /users/eleves-a/2018/jean-charles.layoun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis most frequent hashtags:\n",
    "\n",
    "Following the **map reduce** paradigm to compute the sentiment associated with each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Map by hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_rdd = hashtags.select('filtered', 'hashtag')\\\n",
    "                         .filter(f.col('hashtag').isin(mfht))\\\n",
    "                         .rdd.map(lambda x: (x[1], sid.polarity_scores(' '.join(x[0]))))\n",
    "\n",
    "## Because we want to reduce and aggregate with respect to each event(hashtag), we choose hashtag to be our key:\n",
    "# sentiments_rdd has (key=hashtag, value=sentiment)\n",
    "t1 = time.time()\n",
    "sentiments_rdd.collect()\n",
    "t2 = time.time()\n",
    "time1 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#superbowl', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#btsarmy', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#thebachelor', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#iowacaucuses', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#nowplaying', {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.2732}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#giveaway', {'neg': 0.0, 'neu': 0.743, 'pos': 0.257, 'compound': 0.5859})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce on Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiments(x, y):\n",
    "    dict1 = x\n",
    "    dict2 = y    \n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] += dict2[key]\n",
    "        \n",
    "    return dict1\n",
    "    \n",
    "\n",
    "aggreg = sentiments_rdd.reduceByKey(lambda x, y: aggregate_sentiments(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 4.951999999999999,\n",
       "   'neu': 1030.175000000003,\n",
       "   'pos': 196.87100000000038,\n",
       "   'compound': 349.67990000000117}),\n",
       " ('#arsd',\n",
       "  {'neg': 35.58,\n",
       "   'neu': 451.28099999999995,\n",
       "   'pos': 50.13499999999999,\n",
       "   'compound': 35.2231}),\n",
       " ('#iowa',\n",
       "  {'neg': 73.184,\n",
       "   'neu': 521.3799999999999,\n",
       "   'pos': 101.45099999999998,\n",
       "   'compound': 55.191799999999986}),\n",
       " ('#biggboss',\n",
       "  {'neg': 61.392999999999994,\n",
       "   'neu': 872.8369999999999,\n",
       "   'pos': 221.77900000000002,\n",
       "   'compound': 289.0983999999999}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 133.68300000000002,\n",
       "   'neu': 1076.398,\n",
       "   'pos': 147.925,\n",
       "   'compound': 23.1815})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "event_sentiments_l = aggreg.collect()\n",
    "t2 = time.time()\n",
    "time2 = t2-t1\n",
    "\n",
    "event_sentiments_l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentiment(x):\n",
    "    dict1    = x\n",
    "    sum_prob = dict1['neg'] + dict1['neu'] + dict1['pos']\n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] /= sum_prob\n",
    "    return dict1\n",
    "\n",
    "\n",
    "aggreg_normalized = aggreg.map(lambda x: (x[0], normalize_sentiment(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "event_sentiments_ln = aggreg_normalized.collect()\n",
    "t2 = time.time()\n",
    "time3 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 0.0040194870446218145,\n",
       "   'neu': 0.8361823639324092,\n",
       "   'pos': 0.15979814902296907,\n",
       "   'compound': 0.2838315484278386}),\n",
       " ('#arsd',\n",
       "  {'neg': 0.06625747677822555,\n",
       "   'neu': 0.8403805614939405,\n",
       "   'pos': 0.0933619617278341,\n",
       "   'compound': 0.06559285357805274}),\n",
       " ('#iowa',\n",
       "  {'neg': 0.1051471591847877,\n",
       "   'neu': 0.7490930511555067,\n",
       "   'pos': 0.1457597896597056,\n",
       "   'compound': 0.07929685423446334}),\n",
       " ('#biggboss',\n",
       "  {'neg': 0.05310771801949638,\n",
       "   'neu': 0.7550434295926761,\n",
       "   'pos': 0.19184885238782745,\n",
       "   'compound': 0.2500831740929352}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 0.09844065490137749,\n",
       "   'neu': 0.792631254942909,\n",
       "   'pos': 0.10892809015571361,\n",
       "   'compound': 0.0170702485850578}),\n",
       " ('#asimriaz',\n",
       "  {'neg': 0.06977859288060737,\n",
       "   'neu': 0.7629025185663331,\n",
       "   'pos': 0.16731888855305946,\n",
       "   'compound': 0.15989261982882397}),\n",
       " ('#curecancer',\n",
       "  {'neg': 0.2291470615972054,\n",
       "   'neu': 0.5569524341721126,\n",
       "   'pos': 0.213900504230682,\n",
       "   'compound': -0.09943524859395293}),\n",
       " ('#rashamidesai',\n",
       "  {'neg': 0.06565979381443297,\n",
       "   'neu': 0.7649621993127148,\n",
       "   'pos': 0.16937800687285218,\n",
       "   'compound': 0.17399931271477664})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_sentiments_ln[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments, aggregating and normalizing: 39.36s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments, aggregating and normalizing: {:.4g}s\".format(time1 + time2 + time3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing average sentiment on all tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_rdd  = words_f.select('filtered').rdd.map(lambda x: ' '.join(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sentences = sentences_rdd.collect()\n",
    "t2 = time.time()\n",
    "time4 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "overall_sent = {'neg': 0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "for sentence in sentences:\n",
    "    sentiment = sid.polarity_scores(sentence)    \n",
    "    for key in overall_sent.keys():\n",
    "        overall_sent[key] += sentiment[key]\n",
    "        \n",
    "    \n",
    "for key in overall_sent.keys():\n",
    "        overall_sent[key] /= num_sentences\n",
    "overall_sent\n",
    "t2 = time.time()\n",
    "time5 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments not using an rdd: 82.31s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments not using an rdd: {:.4g}s\".format(time4 + time5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "We can clearly see that using an rdd and utilizing the map reduce scheme gives us faster performance! Also, the naive algorithm above doesn't even group by hashtags..."
   ]
  },
  {
   "source": [
    "### Question 9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We cluster tweets with the k-mean method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+\n|               words|\n+--------------------+\n|[@theythemsbian, ...|\n|[rt, @camillediol...|\n|[rt, @tinyseokjin...|\n|[rt, @ibesuckafre...|\n|[rt, @mikebloombe...|\n+--------------------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o179.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:373)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3343/0x000000010091f840.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3291/0x0000000101010840.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2771/0x000000010123a440.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2776/0x000000010123d440.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2772/0x000000010123b040.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ce3e583b8c0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Form the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata_to_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_words_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filtered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_cluster\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o179.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:373)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3343/0x000000010091f840.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3291/0x0000000101010840.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2771/0x000000010123a440.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2776/0x000000010123d440.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2772/0x000000010123b040.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "clean_word_tokens.select(\"words\").show(5)\n",
    "\n",
    "# Form the data\n",
    "data_to_cluster = exp_words_f.select(\"filtered\").collect()\n",
    "print(data_to_cluster[0])\n",
    "\n",
    "\"\"\"\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(data_to_cluster, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = data_to_cluster.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}