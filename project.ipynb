{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import calendar\n",
    "import datetime\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lower, col, size, length, split\n",
    "from operator import add\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "  .builder \\\n",
    "  .appName(\"Twitter_app\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre',\n",
       " 'http',\n",
       " 'https']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading from the stopword file:\n",
    "text_file = open(\"datasets/stop_words_english.txt\", \"r\")\n",
    "lines = text_file.read()\n",
    "\n",
    "## Creating the stop word array:\n",
    "stopWords = lines.split()\n",
    "# Adding http and https to it:\n",
    "stopWords.append('http')\n",
    "stopWords.append('https')\n",
    "stopWords[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = spark.read.format('json').options(header='true', inferSchema='true') \\\n",
    "  .load('./datasets/NoFilterEnglish2020-02-04.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words preprocessed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------------------------------------------+\n",
      "|text                                                     |\n",
      "+---------------------------------------------------------+\n",
      "|@theythemsbian thank you for being brave enough to say it|\n",
      "+---------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "clean_texts = data.select('*', lower(f.regexp_replace(f.col('text'), r'[^a-zA-Z#@,!\\\\s]', ' ')).alias('text2'))\\\n",
    "                  .drop('text')\\\n",
    "                  .withColumnRenamed('text2', 'text')\n",
    "\n",
    "clean_texts.select('text').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|words_t                                                             |\n",
      "+--------------------------------------------------------------------+\n",
      "|[@theythemsbian, thank, you, for, being, brave, enough, to, say, it]|\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words_t')\n",
    "clean_word_tokens = tokenizer.transform(clean_texts)\n",
    "\n",
    "clean_word_tokens.select('words_t').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove useless words:\n",
    "\n",
    "By useless, we mean the words of size 2 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------+\n",
      "|words                                                       |\n",
      "+------------------------------------------------------------+\n",
      "|[@theythemsbian, thank, you, for, being, brave, enough, say]|\n",
      "+------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Defining the function: (udf: user defined function)\n",
    "filter_length_udf = f.udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "clean_word_tokens = clean_word_tokens.withColumn('words', filter_length_udf(col('words_t')))\n",
    "\n",
    "clean_word_tokens.select('words').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------+\n",
      "|                text|          word|\n",
      "+--------------------+--------------+\n",
      "|@theythemsbian th...|@theythemsbian|\n",
      "|@theythemsbian th...|         thank|\n",
      "|@theythemsbian th...|           you|\n",
      "|@theythemsbian th...|           for|\n",
      "+--------------------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words = clean_word_tokens.withColumn('word', f.explode('words'))\\\n",
    "                             .drop('words')\n",
    "                 \n",
    "#Tokenizer\n",
    "exp_words.select('text', 'word').show(4)\n",
    "\n",
    "exp_words.createOrReplaceTempView(\"words\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreqWords(ts1, ts2, table = \"words\"):\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT word, count(*) AS word_count \" + \n",
    "                      \"FROM {} \".format(table) + \n",
    "                      \"WHERE {0}.timestamp_ms BETWEEN {1} AND {2} \".format(table, ts1, ts2)+ \n",
    "                      \"GROUP BY word \" + \n",
    "                      \"ORDER BY word_count DESC\")\n",
    "    sqlDF.show(10)\n",
    "    return sqlDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| word|word_count|\n",
      "+-----+----------+\n",
      "|  the|        13|\n",
      "|https|         7|\n",
      "|  you|         6|\n",
      "| this|         6|\n",
      "|  for|         4|\n",
      "|  can|         4|\n",
      "| make|         3|\n",
      "| need|         3|\n",
      "|  him|         2|\n",
      "| rare|         2|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 6.186\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time using SPARK only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   13|\n",
      "|https|    7|\n",
      "| this|    6|\n",
      "|  you|    6|\n",
      "|  for|    4|\n",
      "|  can|    4|\n",
      "| need|    3|\n",
      "| make|    3|\n",
      "|  not|    2|\n",
      "| rare|    2|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.504\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "## Timing the operation:\n",
    "t1 = time.time()\n",
    "\n",
    "exp_words.filter(f.col('timestamp_ms').between(ts1, ts2) )\\\n",
    "         .groupBy('word') \\\n",
    "         .count() \\\n",
    "         .sort('count', ascending=False) \\\n",
    "         .show(10)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "\n",
    "We can see that both methods are equivalent in computing time and yield to the same results. Choosing between both is just a question of taste. We Personally prefer **SQL** querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|filtered                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|[@theythemsbian, brave]                                                                                    |\n",
      "|[@camillediola, duterte, violated, law, appointing, honasan,, background,, head, dict, shrugged,, pointing]|\n",
      "|[@tinyseokjinnie, lol, bored, dypgt]                                                                       |\n",
      "|[@ibesuckafree, suck, quitting, weed, argument, amp, gettin]                                               |\n",
      "|[@mikebloomberg, donald, trump, bring, change, country, #superbowl, vciycilow]                             |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing the stopwords from the array of strings\n",
    "sc = spark.sparkContext\n",
    "broadcastVar = sc.broadcast(stopWords)\n",
    "broadcastVar.value\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=broadcastVar.value)\n",
    "words_f = remover.transform(clean_word_tokens)\n",
    "words_f = words_f.drop('words_t', 'words')\n",
    "\n",
    "words_f.select('filtered').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['truncated', 'user', 'withheld_in_countries', 'text', 'filtered']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words_filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------------------------------------------+--------------+\n|text                                                     |word          |\n+---------------------------------------------------------+--------------+\n|@theythemsbian thank you for being brave enough to say it|@theythemsbian|\n+---------------------------------------------------------+--------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the view words_filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+--------------+\n",
      "|text                                                     |word          |\n",
      "+---------------------------------------------------------+--------------+\n",
      "|@theythemsbian thank you for being brave enough to say it|@theythemsbian|\n",
      "+---------------------------------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words_f = words_f.withColumn('word', f.explode('filtered'))#\\\n",
    "                     #.drop('filtered')\n",
    "#.drop('filtered')???\n",
    "\n",
    "exp_words_f.select('text', 'word').show(1, False)\n",
    "\n",
    "exp_words_f.createOrReplaceTempView(\"words_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['user', 'withheld_in_countries', 'text', 'filtered', 'word']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "exp_words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time after filtering using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|         word|word_count|\n",
      "+-------------+----------+\n",
      "|          pro|         2|\n",
      "|         rare|         2|\n",
      "|         rush|         2|\n",
      "|          amp|         2|\n",
      "|         fuck|         2|\n",
      "|    @yungcalc|         1|\n",
      "|    @santeira|         1|\n",
      "|        paral|         1|\n",
      "|@chqmbiedolan|         1|\n",
      "|     whaaaaat|         1|\n",
      "+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.306\n"
     ]
    }
   ],
   "source": [
    "exp_words_f.columns[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting most frequent words in a given period of time after filtering using SPARK SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|           word|word_count|\n",
      "+---------------+----------+\n",
      "|            pro|         2|\n",
      "|            amp|         2|\n",
      "|           rare|         2|\n",
      "|           rush|         2|\n",
      "|           fuck|         2|\n",
      "|          rolls|         1|\n",
      "|       clinging|         1|\n",
      "|@washingtonpost|         1|\n",
      "|        explain|         1|\n",
      "|         donald|         1|\n",
      "+---------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time spent computing: 5.695\n"
     ]
    }
   ],
   "source": [
    "ts1 = 1580770795658\n",
    "ts2 = 1580770796665\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "FreqWords(ts1, ts2, table = \"words_filtered\")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|            word|count|\n",
      "+----------------+-----+\n",
      "|             don| 7913|\n",
      "|             amp| 7470|\n",
      "|          people| 6434|\n",
      "|            love| 6162|\n",
      "|            time| 4881|\n",
      "|             day| 4405|\n",
      "|           trump| 4398|\n",
      "|            good| 4267|\n",
      "|            iowa| 3213|\n",
      "|           today| 3118|\n",
      "|            shit| 3081|\n",
      "|             man| 2949|\n",
      "|           happy| 2346|\n",
      "|            life| 2325|\n",
      "|            fuck| 2290|\n",
      "|           black| 2107|\n",
      "|            year| 2103|\n",
      "|@realdonaldtrump| 2082|\n",
      "|           years| 2062|\n",
      "|           great| 1977|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_words_f.filter(\"(timestamp_ms / 1000 / 60 / 60  % 24 )>= 20\") \\\n",
    "           .groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|          word|          created_at|\n",
      "+--------------+--------------------+\n",
      "|@theythemsbian|Mon Feb 03 22:59:...|\n",
      "|         brave|Mon Feb 03 22:59:...|\n",
      "| @camillediola|Mon Feb 03 22:59:...|\n",
      "|       duterte|Mon Feb 03 22:59:...|\n",
      "|      violated|Mon Feb 03 22:59:...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_time = exp_words_f.select(\"word\", \"created_at\")\n",
    "words_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-----+----+\n",
      "|          word|number|month|hour|\n",
      "+--------------+------+-----+----+\n",
      "|@theythemsbian|    03|    2|  22|\n",
      "|         brave|    03|    2|  22|\n",
      "| @camillediola|    03|    2|  22|\n",
      "|       duterte|    03|    2|  22|\n",
      "|      violated|    03|    2|  22|\n",
      "+--------------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to format month in integer\n",
    "map_month = {'Jan': 1,\n",
    "             'Feb': 2,\n",
    "             'Mar': 3,\n",
    "             'Apr': 4,\n",
    "             'May': 5,\n",
    "             'Jun': 6,\n",
    "             'Jul': 7,\n",
    "             'Aug': 8,\n",
    "             'Sep': 9,\n",
    "             'Oct': 10,\n",
    "             'Nov': 11,\n",
    "             'Dec': 12\n",
    "}\n",
    "\n",
    "udf_function = udf(lambda month: map_month[month], returnType=IntegerType())\n",
    "\n",
    "#Splitting the column created at\n",
    "split_col = split(exp_words_f['created_at'], \" |:\") \n",
    "\n",
    "words_time = words_time.withColumn('day', split_col.getItem(0))\n",
    "words_time = words_time.withColumn('month', udf_function(split_col.getItem(1))) #month represented with irs number\n",
    "words_time = words_time.withColumn('number', split_col.getItem(2))\n",
    "words_time = words_time.withColumn('hour', split_col.getItem(3))\n",
    "words_time = words_time.withColumn('min', split_col.getItem(4))\n",
    "words_time.select(\"word\", \"number\", \"month\", \"hour\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the delta between two date formated as ['number', 'month', 'hour'] in hour\n",
    "def get_delta_hours(start, end):\n",
    "\n",
    "    s = datetime.datetime(year=2020, month=start[1], day=int(start[0]), hour=int(start[2]), minute=0)\n",
    "    e = datetime.datetime(year=2020, month=end[1], day=int(end[0]), hour=int(end[2]), minute=0)\n",
    "\n",
    "    time_delta = e-s\n",
    "    delta_hours = time_delta.days*24 + time_delta.seconds//3600\n",
    "\n",
    "    return delta_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poisson method. We consider that the count of a word is Poisson distributed \n",
    "def poisson_method(df, w, alpha, bucket_size):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    df : dataframe with columns word, month, day, hour\n",
    "    word : word to study\n",
    "    alpha : level of the confidence interval\n",
    "    bucket :size of the bucket to consider (in hour)\n",
    "\n",
    "    Returns a list of eta denoting the unlikeliness of a point\n",
    "    \"\"\"\n",
    "    df = df.filter(df.word==w)\n",
    "\n",
    "    #create a new dataframe with a column count\n",
    "    df = df.groupBy(\"word\", \"number\", \"month\", \"hour\").count()\n",
    "    df = df.orderBy(\"month\", \"number\", \"hour\")\n",
    "    df.show()\n",
    "\n",
    "    df_array = np.array(df.select(\"number\", \"month\", \"hour\",\"count\").collect())\n",
    "\n",
    "    delta_total = get_delta_hours(df_array[0], df_array[len(df_array)-1])\n",
    "\n",
    "    nb_buckets = delta_total//bucket_size\n",
    "    print(nb_buckets)\n",
    "    counts = [0 for i in range(nb_buckets+1)]\n",
    "\n",
    "    for i in range(len(df_array)):\n",
    "        delta_hour = get_delta_hours(df_array[0],df_array[i])\n",
    "        print(delta_hour//bucket_size)\n",
    "        counts[delta_hour//bucket_size]+=int(df_array[i][3])\n",
    "\n",
    "    unlikeliness = []\n",
    "    for i in range(1, len(counts)):\n",
    "        ci_low, ci_high = poisson.interval(alpha, counts[i-1]) \n",
    "        eta = abs(counts[i] -counts[i-1])/(ci_high+0.01) #add a small value to avoid division by zero\n",
    "        unlikeliness.append(eta)\n",
    "\n",
    "    plt.plot(np.arange(1, nb_buckets+1), np.array(unlikeliness), color='red', marker='o')\n",
    "    plt.plot(np.arange(1, nb_buckets+1), counts[1:], color='blue', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-----+----+\n",
      "|          word|number|month|hour|\n",
      "+--------------+------+-----+----+\n",
      "|@theythemsbian|    03|    2|  22|\n",
      "|         brave|    03|    2|  22|\n",
      "| @camillediola|    03|    2|  22|\n",
      "|       duterte|    03|    2|  22|\n",
      "|      violated|    03|    2|  22|\n",
      "+--------------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+------+-----+----+-----+\n",
      "|word|number|month|hour|count|\n",
      "+----+------+-----+----+-----+\n",
      "|test|    03|    2|  23|   41|\n",
      "|test|    04|    2|  00|   43|\n",
      "|test|    04|    2|  01|   33|\n",
      "|test|    04|    2|  02|   49|\n",
      "|test|    04|    2|  03|   49|\n",
      "|test|    04|    2|  04|   53|\n",
      "|test|    04|    2|  05|   47|\n",
      "|test|    04|    2|  06|   44|\n",
      "|test|    04|    2|  07|   42|\n",
      "|test|    04|    2|  08|   32|\n",
      "|test|    04|    2|  09|   40|\n",
      "|test|    04|    2|  10|   38|\n",
      "|test|    04|    2|  11|    7|\n",
      "|test|    04|    2|  12|   29|\n",
      "|test|    04|    2|  13|   25|\n",
      "|test|    04|    2|  14|   23|\n",
      "|test|    04|    2|  15|   32|\n",
      "|test|    04|    2|  16|   68|\n",
      "|test|    04|    2|  17|   66|\n",
      "|test|    04|    2|  18|   71|\n",
      "+----+------+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type numpy.str_)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0a9e5f856c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Apply poisson method with hourly buckets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoisson_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_by_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-b86bf06a0e72>\u001b[0m in \u001b[0;36mpoisson_method\u001b[0;34m(df, w, alpha, bucket_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdf_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"month\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdelta_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_delta_hours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnb_buckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_total\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbucket_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-4cb7fd57b3be>\u001b[0m in \u001b[0;36mget_delta_hours\u001b[0;34m(start, end)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_delta_hours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type numpy.str_)"
     ]
    }
   ],
   "source": [
    "data_by_hour = words_time.select(\"word\", \"number\", \"month\", \"hour\")\n",
    "data_by_hour.show(5)\n",
    "\n",
    "#Apply poisson method with hourly buckets\n",
    "eta = poisson_method(data_by_hour, 'test', 0.95, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = exp_words_f.filter( exp_words_f.word.startswith('#'))\\\n",
    "                      .withColumnRenamed('word', 'hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             hashtag|count|\n",
      "+--------------------+-----+\n",
      "|         #curecancer| 3987|\n",
      "|       #iowacaucuses| 3184|\n",
      "|#deshkiawaazshehnaaz| 2867|\n",
      "|    #mainbhiasimkigf| 2772|\n",
      "|      #meraadarshsid| 2681|\n",
      "|       #iheartawards| 1713|\n",
      "|                #bts| 1654|\n",
      "|           #asimriaz| 1494|\n",
      "|        #thebachelor| 1353|\n",
      "|                 #bb| 1251|\n",
      "|         #iowacaucus| 1249|\n",
      "|     #sidharthshukla| 1235|\n",
      "|           #treasure| 1232|\n",
      "|        #coronavirus| 1155|\n",
      "|           #biggboss| 1155|\n",
      "|           #iacaucus| 1134|\n",
      "|         #loveisland|  996|\n",
      "|          #superbowl|  962|\n",
      "|     #worldcancerday|  863|\n",
      "|               #sotu|  842|\n",
      "|     #rightchoicesid|  839|\n",
      "|              #yikes|  758|\n",
      "|     #happyjisungday|  706|\n",
      "|         #nowplaying|  646|\n",
      "|            #btsarmy|  646|\n",
      "|                #nct|  640|\n",
      "|             #superm|  573|\n",
      "|               #iowa|  572|\n",
      "|                #raw|  565|\n",
      "|       #rashamidesai|  564|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_freq_hash = hashtags.groupBy('hashtag') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False) \n",
    "most_freq_hash.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting Hashtag Data Frame to array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfht = most_freq_hash.filter('count >= 500').select('hashtag').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting words related to Hashtag example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#iowacaucuses',\n",
       " 'iowa',\n",
       " 'bernie',\n",
       " 'caucus',\n",
       " 'vote',\n",
       " 'won',\n",
       " 'tonight',\n",
       " '#iowacaucus',\n",
       " 'biden',\n",
       " 'app',\n",
       " '@donaldjtrumpjr',\n",
       " 'dnc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_words = exp_words_f.filter( f.col('text').contains('#iowacaucuses'))\n",
    "\n",
    "event_words.groupBy('word') \\\n",
    "           .count() \\\n",
    "           .sort('count', ascending=False)\\\n",
    "           .rdd\\\n",
    "           .map(lambda x: x[0])\\\n",
    "           .collect()[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex5 using pySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----+\n",
      "|        hashtag|       word|count|\n",
      "+---------------+-----------+-----+\n",
      "|         #yikes|@nickiminaj|  451|\n",
      "|         #yikes|      likes|  344|\n",
      "|         #yikes|        amp|  336|\n",
      "|         #yikes|       play|  325|\n",
      "|         #yikes|        tag|  313|\n",
      "|         #yikes|       life|  311|\n",
      "|         #yikes|    yzdifvf|  310|\n",
      "|         #yikes|       rosa|  130|\n",
      "|         #yikes|      nicki|  111|\n",
      "|#worldcancerday|     cancer|  347|\n",
      "+---------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ex.5: Get the most frequent words for each event:\n",
    "ht_f = hashtags.select('filtered', 'hashtag')\\\n",
    "               .filter(f.col('hashtag').isin(mfht))\\\n",
    "               .withColumn('word', f.explode('filtered'))\\\n",
    "               .filter(~col('word').startswith('#'))\\\n",
    "               .select('hashtag', 'word')\\\n",
    "               .groupBy('hashtag', 'word')\\\n",
    "               .count()\\\n",
    "               .sort(['hashtag', 'count'], ascending=False)\\\n",
    "               .filter('count >= 100')\n",
    "\n",
    "ht_f.show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the top 10 most frequent words by events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|    hashtag|       word|count|\n",
      "+-----------+-----------+-----+\n",
      "|#curecancer|trueworship| 3951|\n",
      "|#curecancer|     cancer| 2441|\n",
      "|#curecancer|       true| 1240|\n",
      "|#curecancer|     rampal| 1073|\n",
      "|#curecancer|    worship| 1042|\n",
      "|#curecancer|      saint| 1040|\n",
      "|#curecancer|       cure|  916|\n",
      "|#curecancer|        god|  884|\n",
      "|#curecancer|    maharaj|  859|\n",
      "|#curecancer|   diseases|  846|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+---------------+-----+\n",
      "|      hashtag|           word|count|\n",
      "+-------------+---------------+-----+\n",
      "|#iowacaucuses|           iowa|  499|\n",
      "|#iowacaucuses|         bernie|  346|\n",
      "|#iowacaucuses|         caucus|  237|\n",
      "|#iowacaucuses|           vote|  193|\n",
      "|#iowacaucuses|            won|  173|\n",
      "|#iowacaucuses|        tonight|  155|\n",
      "|#iowacaucuses|          biden|  153|\n",
      "|#iowacaucuses|            app|  151|\n",
      "|#iowacaucuses|@donaldjtrumpjr|  150|\n",
      "|#iowacaucuses|            dnc|  147|\n",
      "+-------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+-----+\n",
      "|             hashtag|     word|count|\n",
      "+--------------------+---------+-----+\n",
      "|#deshkiawaazshehnaaz|     sana|  459|\n",
      "|#deshkiawaazshehnaaz|     love|  287|\n",
      "|#deshkiawaazshehnaaz|     guys|  235|\n",
      "|#deshkiawaazshehnaaz| shehnaaz|  193|\n",
      "|#deshkiawaazshehnaaz|    trend|  143|\n",
      "|#deshkiawaazshehnaaz|@colorstv|  141|\n",
      "|#deshkiawaazshehnaaz|    speed|  128|\n",
      "|#deshkiawaazshehnaaz| trending|  119|\n",
      "|#deshkiawaazshehnaaz|   follow|  116|\n",
      "|#deshkiawaazshehnaaz|      hai|  113|\n",
      "+--------------------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-----------+-----+\n",
      "|         hashtag|       word|count|\n",
      "+----------------+-----------+-----+\n",
      "|#mainbhiasimkigf|       asim|  982|\n",
      "|#mainbhiasimkigf|      trend|  485|\n",
      "|#mainbhiasimkigf|      girls|  482|\n",
      "|#mainbhiasimkigf|   trending|  291|\n",
      "|#mainbhiasimkigf|  @colorstv|  253|\n",
      "|#mainbhiasimkigf|       love|  214|\n",
      "|#mainbhiasimkigf|   fangirls|  202|\n",
      "|#mainbhiasimkigf|        fan|  193|\n",
      "|#mainbhiasimkigf|  @biggboss|  187|\n",
      "|#mainbhiasimkigf|@imrealasim|  186|\n",
      "+----------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+---------------+-----+\n",
      "|       hashtag|           word|count|\n",
      "+--------------+---------------+-----+\n",
      "|#meraadarshsid|            sid|  516|\n",
      "|#meraadarshsid|         shukla|  428|\n",
      "|#meraadarshsid|      @sidharth|  352|\n",
      "|#meraadarshsid|         tweets|  282|\n",
      "|#meraadarshsid|@realvindusingh|  281|\n",
      "|#meraadarshsid|          today|  228|\n",
      "|#meraadarshsid|            tag|  223|\n",
      "|#meraadarshsid|      @biggboss|  180|\n",
      "|#meraadarshsid|       activity|  162|\n",
      "|#meraadarshsid|        tagline|  160|\n",
      "+--------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+----------------+-----+\n",
      "|      hashtag|            word|count|\n",
      "+-------------+----------------+-----+\n",
      "|#iheartawards|            @bts|  567|\n",
      "|#iheartawards|             twt|  553|\n",
      "|#iheartawards|            vote|  536|\n",
      "|#iheartawards|             sos|  264|\n",
      "|#iheartawards|   @justinbieber|  259|\n",
      "|#iheartawards|     @danandshay|  226|\n",
      "|#iheartawards|            @tha|  220|\n",
      "|#iheartawards|@ygofficialblink|  216|\n",
      "|#iheartawards|          voting|  200|\n",
      "|#iheartawards|           hours|  187|\n",
      "+-------------+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+----------+-----+\n",
      "|hashtag|      word|count|\n",
      "+-------+----------+-----+\n",
      "|   #bts|      @bts|  739|\n",
      "|   #bts|       twt|  525|\n",
      "|   #bts|      soul|  403|\n",
      "|   #bts|     outro|  368|\n",
      "|   #bts|  comeback|  357|\n",
      "|   #bts|   trailer|  355|\n",
      "|   #bts|@bighitent|  305|\n",
      "|   #bts|       map|  297|\n",
      "|   #bts|       ego|  225|\n",
      "|   #bts|     weeks|  218|\n",
      "+-------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+----+-----+\n",
      "|  hashtag|word|count|\n",
      "+---------+----+-----+\n",
      "|#asimriaz|asim|  334|\n",
      "|#asimriaz|fans|  218|\n",
      "|#asimriaz| amp|  208|\n",
      "|#asimriaz|love|  104|\n",
      "+---------+----+-----+\n",
      "\n",
      "+------------+------+-----+\n",
      "|     hashtag|  word|count|\n",
      "+------------+------+-----+\n",
      "|#thebachelor| peter|  286|\n",
      "|#thebachelor|kelley|  130|\n",
      "|#thebachelor|kelsey|  128|\n",
      "|#thebachelor| tammy|  113|\n",
      "+------------+------+-----+\n",
      "\n",
      "+-----------+------+-----+\n",
      "|    hashtag|  word|count|\n",
      "+-----------+------+-----+\n",
      "|#iowacaucus|  iowa|  226|\n",
      "|#iowacaucus|bernie|  148|\n",
      "|#iowacaucus|caucus|  127|\n",
      "+-----------+------+-----+\n",
      "\n",
      "+-------+---------+-----+\n",
      "|hashtag|     word|count|\n",
      "+-------+---------+-----+\n",
      "|    #bb|   shukla|  211|\n",
      "|    #bb|      sid|  134|\n",
      "|    #bb|@sidharth|  125|\n",
      "|    #bb|     asim|  121|\n",
      "|    #bb|@colorstv|  105|\n",
      "|    #bb|      amp|  104|\n",
      "+-------+---------+-----+\n",
      "\n",
      "+---------------+---------+-----+\n",
      "|        hashtag|     word|count|\n",
      "+---------------+---------+-----+\n",
      "|#sidharthshukla|      amp|  206|\n",
      "|#sidharthshukla|   shukla|  187|\n",
      "|#sidharthshukla|@sidharth|  147|\n",
      "|#sidharthshukla|      sid|  137|\n",
      "|#sidharthshukla|      hai|  104|\n",
      "+---------------+---------+-----+\n",
      "\n",
      "+---------+-----------+-----+\n",
      "|  hashtag|       word|count|\n",
      "+---------+-----------+-----+\n",
      "|#treasure|  editorial| 2316|\n",
      "|#treasure|   treasure| 1178|\n",
      "|#treasure|       week| 1160|\n",
      "|#treasure|      maker| 1160|\n",
      "|#treasure|photography| 1158|\n",
      "|#treasure|   official| 1148|\n",
      "|#treasure|     @ygent| 1148|\n",
      "|#treasure|       bang|  373|\n",
      "|#treasure|        dam|  373|\n",
      "|#treasure|      asahi|  256|\n",
      "+---------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+--------------+-----+\n",
      "|  hashtag|          word|count|\n",
      "+---------+--------------+-----+\n",
      "|#iacaucus|          iowa|  221|\n",
      "|#iacaucus| @andrewgillum|  141|\n",
      "|#iacaucus|@gopchairwoman|  139|\n",
      "|#iacaucus|          wait|  129|\n",
      "|#iacaucus|     precincts|  122|\n",
      "|#iacaucus|        delays|  121|\n",
      "|#iacaucus|          hear|  115|\n",
      "|#iacaucus|           bad|  115|\n",
      "|#iacaucus|        winner|  113|\n",
      "|#iacaucus|         black|  113|\n",
      "+---------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+---------+-----+\n",
      "|     hashtag|     word|count|\n",
      "+------------+---------+-----+\n",
      "|#coronavirus|    china|  238|\n",
      "|#coronavirus|     evil|  153|\n",
      "|#coronavirus|    cases|  133|\n",
      "|#coronavirus|confirmed|  103|\n",
      "+------------+---------+-----+\n",
      "\n",
      "+---------+---------+-----+\n",
      "|  hashtag|     word|count|\n",
      "+---------+---------+-----+\n",
      "|#biggboss|     asim|  173|\n",
      "|#biggboss|@colorstv|  151|\n",
      "|#biggboss|     boss|  134|\n",
      "|#biggboss|      amp|  131|\n",
      "|#biggboss|     fans|  131|\n",
      "|#biggboss|     bigg|  130|\n",
      "|#biggboss|     riaz|  129|\n",
      "|#biggboss|   shukla|  120|\n",
      "+---------+---------+-----+\n",
      "\n",
      "+-----------+--------+-----+\n",
      "|    hashtag|    word|count|\n",
      "+-----------+--------+-----+\n",
      "|#loveisland|  callum|  240|\n",
      "|#loveisland|shaughna|  117|\n",
      "+-----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-18df6455be52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mht\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmfht\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mht_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hashtag'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/pls_do_not_delete/miniconda3/envs/inf554/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    ht_f.filter(col('hashtag').isin(ht)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6-7 with SparkRDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because we want to reduce and aggregate with respect to each event(hashtag), we choose hashtag to be our key:\n",
    "# ht_rdd has (key=hashtag, value=(min(timestamp_ms), max(timestamp_ms), place.bounding_box.coordinates, number_of_places))\n",
    "ht_rdd = hashtags.select('timestamp_ms', 'place.bounding_box.coordinates', 'hashtag')\\\n",
    "                 .filter(f.col('hashtag').isin(mfht))\\\n",
    "                 .rdd.map(lambda x: (x[2], (x[0], x[0], x[1], 0 if x[1] is None else 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate function for finding minimum, maximum timeframe and the mean coordinates of each Event    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ts_place(x, y):\n",
    "    # for calculating minimum timeframe\n",
    "    ts1 = int(x[0])\n",
    "    ts2 = int(y[0])\n",
    "    # for calculating minimum timeframe    \n",
    "    ts3 = int(x[1])\n",
    "    ts4 = int(y[1])\n",
    "    \n",
    "    # for summing the coordinates\n",
    "    place1 = x[2]\n",
    "    place2 = y[2]\n",
    "    \n",
    "    # number of places of Event, needed for calculating the mean of the coordinates\n",
    "    increment = 0\n",
    "    \n",
    "    # Considering that most of the Tweets don't have the coordinates\n",
    "    # Aggregate function will select:\n",
    "    # - None, in case both of tweets do not have\n",
    "    if(place1 is None):\n",
    "        if(place2 is None):\n",
    "            place = None\n",
    "    # - Coordinates of one, if only one of the has  \n",
    "        else:\n",
    "            place = place2\n",
    "            increment = y[3]\n",
    "        \n",
    "    elif(place2 is None):\n",
    "        place = place1\n",
    "        increment = x[3]\n",
    "    # - Sum of the coordinates if both of them have\n",
    "    else:\n",
    "        place = np.array(place1) + np.array(place2)\n",
    "        place = place.squeeze()\n",
    "        increment = x[3] + y[3]\n",
    "    # Finally returns, min/max timeframe, coordinates of the place, and number of places for each Event        \n",
    "    return (min(ts3, ts4), max(ts1, ts2), place, increment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We now need to compute the means of all places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the Aggregation function\n",
    "aggreg_ht = ht_rdd.reduceByKey(lambda x, y: aggregate_ts_place(x, y))\n",
    "aggreg_htn = aggreg_ht.map(lambda x: (x[0], x[1][0], x[1][1], np.array(x[1][2])/x[1][3] if x[1][3] != 0 else x[1][3], x[1][3]))\n",
    "ts_loc_l = aggreg_htn.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that converts geolocalisation to place name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(coords):\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")  \n",
    "    if coords.shape != (2,):\n",
    "        coords = coords.mean(axis = 0)\n",
    "    \n",
    "    Longitude = str(coords[0])\n",
    "    Latitude = str(coords[1])\n",
    "    \n",
    "    location = geolocator.reverse(Latitude+\",\"+Longitude)\n",
    "    \n",
    "    if(location == None):\n",
    "        print(\"\\tNo location specified\")\n",
    "        return\n",
    " \n",
    "    address = location.raw['address']\n",
    "\n",
    "    # traverse the data\n",
    "    city = address.get('city', '')\n",
    "    state = address.get('state', '')\n",
    "    country = address.get('country', '')\n",
    "    code = address.get('country_code')\n",
    "    zipcode = address.get('postcode')\n",
    "    print('\\tCity : ', city)\n",
    "    print('\\tState : ', state)\n",
    "    print('\\tCountry : ', country)\n",
    "    print('\\tZip Code : ', zipcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT: #treasure\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:21:02 to - 2020-02-04 22:47:25\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #arsd\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 21:54:36 to - 2020-02-04 22:54:08\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowa\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:33:04 to - 2020-02-04 22:57:14\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Oklahoma\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #biggboss\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:17 to - 2020-02-04 22:51:09\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Telangana\n",
      "\tCountry :  India\n",
      "\tZip Code :  500089\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #thebachelor\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:18:15 to - 2020-02-04 22:35:01\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Kentucky\n",
      "\tCountry :  United States\n",
      "\tZip Code :  42633\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #asimriaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:11 to - 2020-02-04 22:54:44\n",
      "\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #curecancer\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:26:22 to - 2020-02-04 22:40:42\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Madhya Pradesh\n",
      "\tCountry :  India\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #rashamidesai\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:29:18 to - 2020-02-04 22:54:19\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:55:19 to - 2020-02-04 22:59:43\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Indiana\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #sidharthshukla\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:25:15 to - 2020-02-04 22:55:59\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #bts\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:59:09 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Virginia\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #raw\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:18:30 to - 2020-02-04 22:45:35\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #rightchoicesid\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 20:34:15 to - 2020-02-04 21:47:09\n",
      "\n",
      "\tCity :  Karnal\n",
      "\tState :  Haryana\n",
      "\tCountry :  India\n",
      "\tZip Code :  132001\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #coronavirus\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:58:49 to - 2020-02-04 22:59:29\n",
      "\n",
      "\tCity :  \n",
      "\tState :  كوركول\n",
      "\tCountry :  موريتانيا\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #nct\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:54:59 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #superbowl\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:29:25 to - 2020-02-04 22:46:36\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowacaucuses\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:41 to - 2020-02-04 22:57:53\n",
      "\n",
      "\tCity :  \n",
      "\tState :  West Virginia\n",
      "\tCountry :  United States\n",
      "\tZip Code :  25270\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #nowplaying\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:30:38 to - 2020-02-04 22:55:34\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #worldcancerday\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:28:44 to - 2020-02-04 22:53:51\n",
      "\n",
      "\tCity :  Sidi Ghanem سيدي غانم\n",
      "\tState :  \n",
      "\tCountry :  Maroc / ⵍⵎⵖⵔⵉⴱ / المغرب\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iowacaucusdisaster\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:57:07 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Arkansas\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iheartawards\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:33:44 to - 2020-02-04 22:58:24\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #iacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:25 to - 2020-02-04 22:57:49\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  65274\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #superm\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:05 to - 2020-02-04 22:53:47\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #yikes\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:28:22 to - 2020-02-04 22:54:43\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #meraadarshsid\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:17 to - 2020-02-04 22:58:41\n",
      "\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #loveisland\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:58:52 to - 2020-02-04 22:59:25\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #sb\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:24:20 to - 2020-02-04 22:58:54\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #shehnaazgill\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:17:44 to - 2020-02-04 22:46:59\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #btsarmy\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:29:35 to - 2020-02-04 22:58:24\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Wisconsin\n",
      "\tCountry :  United States\n",
      "\tZip Code :  54241\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #mainbhiasimkigf\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:33:58 to - 2020-02-04 22:44:25\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #deshkiawaazshehnaaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:30:36 to - 2020-02-04 22:56:56\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  431201\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #trump\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:31:00 to - 2020-02-04 22:57:45\n",
      "\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  27540\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #sotu\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:58:40 to - 2020-02-04 22:59:37\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #happyjisungday\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:59:01 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tNo location specified\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "EVENT: #bb\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 22:34:23 to - 2020-02-04 22:56:17\n",
      "\n",
      "\tCity :  \n",
      "\tState :  Telangana\n",
      "\tCountry :  India\n",
      "\tZip Code :  500089\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for event in ts_loc_l:\n",
    "    print(\"EVENT: \" + event[0]+'\\n')\n",
    "    start = str(pd.to_datetime(event[1], unit='ms').to_pydatetime())\n",
    "    end = str( pd.to_datetime(event[2], unit='ms').to_pydatetime())\n",
    "    print(\"\\tTimeframe : from - \" + start[:-7] + \" to - \" + end[:-7]+'\\n')\n",
    "    \n",
    "    if(np.sum(event[4]) == 0 ):\n",
    "        print(\"\\tNo location specified\")\n",
    "    else: \n",
    "        loc(event[3].squeeze())\n",
    "    print(\"\\n-----------------------------------------------------------------------\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 and Exercice 7: with for loop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: #curecancer\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 03:16:13 to - 2020-02-04 22:42:39\n",
      "\n",
      "\tMost frequent Location is : karnal, India\n",
      "\tCity :  \n",
      "\tState :  Madhya Pradesh\n",
      "\tCountry :  India\n",
      "\tZip Code :  476332\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['trueworship', 'cancer', 'true', 'rampal', 'saint', 'worship', 'cure', 'god']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowacaucuses\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:58:28\n",
      "\n",
      "\tMost frequent Location is : Manhattan, United States\n",
      "\tCity :  \n",
      "\tState :  West Virginia\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', 'bernie', 'caucus', 'vote', 'won', 'app', '@donaldjtrumpjr', 'party']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #deshkiawaazshehnaaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 01:58:49 to - 2020-02-04 22:59:12\n",
      "\n",
      "\tMost frequent Location is : Aurangabad, India\n",
      "\tCity :  \n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  431201\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['sana', 'love', 'guys', 'shehnaaz', 'trend', '@colorstv', 'speed', 'follow']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #mainbhiasimkigf\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:26:18 to - 2020-02-04 22:58:06\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['asim', 'trend', 'girls', 'trending', '@colorstv', 'love', 'fangirls', 'fan']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #meraadarshsid\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 08:30:32 to - 2020-02-04 22:59:16\n",
      "\n",
      "\tMost frequent Location is : Mumbai, India\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['sid', 'shukla', '@sidharth', '@realvindusingh', 'tweets', 'tag', 'today', '@biggboss']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iheartawards\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:38 to - 2020-02-04 22:58:29\n",
      "\n",
      "\tMost frequent Location is : Sheffield, United Kingdom\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'vote', 'sos', '@justinbieber', '@danandshay', '@tha', '@ygofficialblink']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #bts\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:59:54\n",
      "\n",
      "\tMost frequent Location is : Miami Shores, United States\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'bts', 'soul', 'outro', 'comeback', 'trailer', '@bighitent']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #asimriaz\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:59:07\n",
      "\n",
      "\tMost frequent Location is : Mumbai, India\n",
      "\tCity :  Mumbai\n",
      "\tState :  Maharashtra\n",
      "\tCountry :  India\n",
      "\tZip Code :  400065\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['asim', 'amp', 'fans', 'love', 'vote', 'media', '@biggboss', '@thekhbri']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #thebachelor\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:54:09\n",
      "\n",
      "\tMost frequent Location is : Austin, United States\n",
      "\tCity :  \n",
      "\tState :  Kentucky\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['peter', 'kelley', 'kelsey', 'tammy', 'mykenna', 'drama', 'bachelor', 'girls']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #bb\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:05:20 to - 2020-02-04 22:58:37\n",
      "\n",
      "\tMost frequent Location is : South Africa, South Africa\n",
      "\tCity :  \n",
      "\tState :  Kogi\n",
      "\tCountry :  Nigeria\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['shukla', 'prior', 'pyong', '@cleytu', 'tweet', 'appreciation', 'thnbpfwynp', 'jantando']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tMost frequent Location is : Des Moines, United States\n",
      "\tCity :  \n",
      "\tState :  Ohio\n",
      "\tCountry :  United States\n",
      "\tZip Code :  45662\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', 'bernie', 'caucus', 'app', 'breaking', 'vote', 'democrats', 'party']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #sidharthshukla\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:15:22 to - 2020-02-04 22:56:29\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['amp', 'shukla', '@sidharth', 'sid', 'hai', 'sidharth', 'boss', 'bigg']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #treasure\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:44 to - 2020-02-04 22:47:25\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['editorial', 'treasure', 'week', 'maker', 'photography', 'official', '@ygent', 'bang']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #coronavirus\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:59:29\n",
      "\n",
      "\tMost frequent Location is : Honolulu, United States\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['china', 'evil', 'cases', 'chinese', 'confirmed', 'medical', 'patients', 'hospital']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #biggboss\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:13 to - 2020-02-04 22:56:29\n",
      "\n",
      "\tMost frequent Location is : Rajendra Nagar, India\n",
      "\tCity :  \n",
      "\tState :  Telangana\n",
      "\tCountry :  India\n",
      "\tZip Code :  500089\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['asim', '@colorstv', 'amp', 'fans', 'riaz', 'boss', 'bigg', 'shukla']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iacaucus\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:04:16 to - 2020-02-04 22:59:04\n",
      "\n",
      "\tMost frequent Location is : Des Moines, United States\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', '@andrewgillum', '@gopchairwoman', 'wait', 'precincts', 'delays', 'hear', 'winner']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #loveisland\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:30 to - 2020-02-04 22:59:25\n",
      "\n",
      "\tMost frequent Location is : Brent, United Kingdom\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['callum', 'shaughna', 'casa', 'mike', 'amor', 'ched', 'villa', 'girl']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #superbowl\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 22:59:55 to - 2020-02-04 22:52:53\n",
      "\n",
      "\tMost frequent Location is : Houston, United States\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['trump', 'anthem', 'national', 'halftime', 'mocking', '@shakira', 'stand', 'caught']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #worldcancerday\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:29:10 to - 2020-02-04 22:58:40\n",
      "\n",
      "\tMost frequent Location is : God's Favourite House, Nigeria\n",
      "\tCity :  Sidi Ghanem سيدي غانم\n",
      "\tState :  \n",
      "\tCountry :  Maroc / ⵍⵎⵖⵔⵉⴱ / المغرب\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['cancer', 'today', 'amp', 'people', 'day', 'awareness', 'support', 'disease']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #sotu\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:08:18 to - 2020-02-04 22:59:43\n",
      "\n",
      "\tMost frequent Location is : Florida, United States\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['tomorrow', 'trump', 'attend', 'tonight', 'join', 'people', '@potus', '@flotus']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #rightchoicesid\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:39 to - 2020-02-04 21:47:09\n",
      "\n",
      "\tMost frequent Location is : karnal, India\n",
      "\tCity :  Karnal\n",
      "\tState :  Haryana\n",
      "\tCountry :  India\n",
      "\tZip Code :  132001\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['sid', 'shukla', '@sidharth', '@realvindusingh', 'guys', 'trend', 'win', '@biggboss']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #yikes\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 03:15:25 to - 2020-02-04 22:57:35\n",
      "\n",
      "\tMost frequent Location is : Bryn Mawr, United States\n",
      "\tCity :  \n",
      "\tState :  Kansas\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@nickiminaj', 'likes', 'amp', 'play', 'life', 'tag', 'yzdifvf', 'nicki']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #happyjisungday\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 03:27:07 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['birthday', 'happy', '@nctsmtown', 'slthg', 'jisung', 'dream', 'park', 'baby']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #nowplaying\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:17 to - 2020-02-04 22:56:54\n",
      "\n",
      "\tMost frequent Location is : Potsdam, Deutschland\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['listen', 'radio', 'live', 'amp', 'tune', 'love', 'feat', 'music']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #btsarmy\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:02 to - 2020-02-04 22:58:56\n",
      "\n",
      "\tMost frequent Location is : Two Rivers, United States\n",
      "\tCity :  \n",
      "\tState :  Wisconsin\n",
      "\tCountry :  United States\n",
      "\tZip Code :  54241\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'message', 'bts', '@choi', 'album', 'era', '@btsvotingteam']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #nct\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:09:25 to - 2020-02-04 22:59:53\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['birthday', 'happy', '@nctsmtown', 'slthg', 'dream', 'icn', 'superm', 'live']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #superm\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:58 to - 2020-02-04 22:58:32\n",
      "\n",
      "\tMost frequent Location is : Maryland, United States\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  27239:27292\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['live', '@superm', 'superm', 'san', 'future', 'jose', 'sjc', 'hhelen']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowa\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:11 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tMost frequent Location is : Des Moines, United States\n",
      "\tCity :  \n",
      "\tState :  Kentucky\n",
      "\tCountry :  United States\n",
      "\tZip Code :  41006\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['iowa', 'bernie', 'caucus', 'app', 'democrats', 'breaking', 'vote', 'party']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #raw\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:07:33 to - 2020-02-04 22:56:17\n",
      "\n",
      "\tMost frequent Location is : Mayville, United States\n",
      "\tCity :  \n",
      "\tState :  Missouri\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@wwe', 'wwe', '@wweuniverse', '@randyorton', '@wweasuka', 'challenge', '@angelgarzawwe', '@beckylynchwwe']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #rashamidesai\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:03:45 to - 2020-02-04 22:58:18\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['amp', 'khabri', '@biggboss', '@real', '@therashamidesai', 'elite', 'breaking', 'rashami']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #sb\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:37 to - 2020-02-04 22:58:54\n",
      "\n",
      "\tMost frequent Location is : Armagh, United Kingdom\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@sb', 'official', 'billboardmainstay', '@mor', 'alab', 'onawesamsungcon', '@nfl', '@jlo']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #arsd\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:01:05 to - 2020-02-04 22:54:08\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@bts', 'twt', 'drop', 'hype', 'arsd', 'love', '@drwnk', 'zvrnb']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #iowacaucusdisaster\n",
      "\n",
      "\tTimeframe : from - 2020-02-04 04:46:17 to - 2020-02-04 22:59:52\n",
      "\n",
      "\tMost frequent Location is : Corinth, United States\n",
      "\tCity :  \n",
      "\tState :  Tennessee\n",
      "\tCountry :  United States\n",
      "\tZip Code :  38482\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['democrats', 'iowa', 'cnn', 'party', 'dnc', 'crying', 'died', 'betrayed']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #shehnaazgill\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:02:57 to - 2020-02-04 22:55:40\n",
      "\n",
      "\tMost frequent Location is : No Location specified\n",
      "\tNo location specified\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['@colorstv', '@beingsalmankhan', 'shukla', 'india', 'hai', '@vivo', 'vote', 'asim']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "Event: #trump\n",
      "\n",
      "\tTimeframe : from - 2020-02-03 23:00:03 to - 2020-02-04 22:58:58\n",
      "\n",
      "\tMost frequent Location is : Antioch, United States\n",
      "\tCity :  \n",
      "\tState :  North Carolina\n",
      "\tCountry :  United States\n",
      "\tZip Code :  None\n",
      "\n",
      "\tMost frequent words associated to the event: \n",
      "\t['trump', '@realdonaldtrump', '@funder', 'landslide', '@joncoopertweets', 'hand', 'raise', 'schiff']\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ht in mfht:\n",
    "    event_words = exp_words_f.filter( f.col('text').contains(ht))\n",
    "    event_texts = clean_texts.filter( f.col('text').contains(ht))\n",
    "\n",
    "    ## Ex.5: Get the most frequent words for each event:\n",
    "    most_freq_w = event_words.filter(~col('word').startswith('#'))\\\n",
    "                             .groupBy('word') \\\n",
    "                             .count() \\\n",
    "                             .sort('count', ascending=False)\\\n",
    "                             .rdd\\\n",
    "                             .map(lambda x: x[0])\\\n",
    "                             .collect()[:8]    \n",
    "    \n",
    "    # Ex.6 : Finding timeframe of the Event\n",
    "    end   = event_texts.select(f.max(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    start = event_texts.select(f.min(f.col('timestamp_ms'))).rdd.map(list).collect()[0][0]\n",
    "    \n",
    "    # Ex.7 : Finding location of the Event\n",
    "    coords = event_texts.na.drop(subset=[\"place\"]) \\\n",
    "                        .select(\"place.bounding_box.coordinates\") \\\n",
    "                        .rdd.map(list).collect()\n",
    "    \n",
    "    ## Gets the most common location from the tweets !\n",
    "    location_name = event_texts.na.drop(subset=[\"place\"]) \\\n",
    "                               .groupBy('place.name', 'place.country') \\\n",
    "                               .count()\\\n",
    "                               .sort('count', ascending=False)\\\n",
    "                               .rdd\\\n",
    "                               .map(lambda x : x[0] + ', ' + x[1])\\\n",
    "                               .collect()\n",
    "    \n",
    "    ####### Printing the results\n",
    "    \n",
    "    print(\"EVENT: \" + ht+'\\n')\n",
    "    \n",
    "    end = str( pd.to_datetime(end, unit='ms').to_pydatetime())\n",
    "    start = str(pd.to_datetime(start, unit='ms').to_pydatetime())\n",
    "    print(\"\\tTimeframe : from - \" + start[:-7] + \" to - \" + end[:-7]+'\\n')\n",
    "    \n",
    "    # Printing the location found by the first method: The Most Common Location\n",
    "    if(location_name != []):\n",
    "        location_name = location_name[0]\n",
    "    else:\n",
    "        location_name = 'No Location specified'\n",
    "\n",
    "    print(\"\\tMost frequent Location is : \" + location_name)\n",
    "    \n",
    "    # Printing the location found by the second method: The Mean Of Coordinates\n",
    "    if(coords != []):\n",
    "        coords_mean = np.array(coords).squeeze().mean(axis = 0)\n",
    "        loc(coords_mean)\n",
    "    else: \n",
    "        print(\"\\tNo location specified\")\n",
    "    \n",
    "    # Printing most frequent words\n",
    "    if(most_freq_w == []):\n",
    "        most_freq_w = 'no words found...'\n",
    "    \n",
    "\n",
    "    print(\"\\n\\tMost frequent words associated to the event: \\n\\t\" + str(most_freq_w))\n",
    "    print(\"\\n------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Benchmarking of the different methods for finding minimum / maximum timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "time spent computing: 6.778\n",
      "Method 2\n",
      "time spent computing: 6.634\n",
      "Method 3\n",
      "time spent computing: 6.558\n",
      "Method 4\n",
      "time spent computing: 0.009586\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Use describe()\n",
    "t1 = time.time()\n",
    "float(event_words.describe(\"timestamp_ms\").filter(\"summary = 'max'\").select(\"timestamp_ms\").collect()[0].asDict()['timestamp_ms'])\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 1\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 2: Use SQL\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.registerTempTable(\"df_table\")\n",
    "spark.sql(\"SELECT MAX(timestamp_ms) as maxval FROM df_table\").collect()[0].asDict()['maxval']\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 2\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "# Method 3: Convert to RDD\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(\"timestamp_ms\").rdd.max()[0]\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 3\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))\n",
    "\n",
    "\n",
    "# Method 4: select\n",
    "t1 = time.time()\n",
    "\n",
    "event_words.select(f.max(f.col('timestamp_ms')))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Method 4\")\n",
    "print(\"time spent computing: {:.4g}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "\n",
    "Use NLP package from [nltk](https://www.nltk.org/api/nltk.sentiment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /users/eleves-a/2018/jean-charles.layoun/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis most frequent hashtags:\n",
    "\n",
    "Following the **map reduce** paradigm to compute the sentiment associated with each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Map by hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_rdd = hashtags.select('filtered', 'hashtag')\\\n",
    "                         .filter(f.col('hashtag').isin(mfht))\\\n",
    "                         .rdd.map(lambda x: (x[1], sid.polarity_scores(' '.join(x[0]))))\n",
    "\n",
    "## Because we want to reduce and aggregate with respect to each event(hashtag), we choose hashtag to be our key:\n",
    "# sentiments_rdd has (key=hashtag, value=sentiment)\n",
    "t1 = time.time()\n",
    "sentiments_rdd.collect()\n",
    "t2 = time.time()\n",
    "time1 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#superbowl', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#btsarmy', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}),\n",
       " ('#coronavirus', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#thebachelor', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#iowacaucuses', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#nowplaying', {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.2732}),\n",
       " ('#bts', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('#giveaway', {'neg': 0.0, 'neu': 0.743, 'pos': 0.257, 'compound': 0.5859})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reduce on Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiments(x, y):\n",
    "    dict1 = x\n",
    "    dict2 = y    \n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] += dict2[key]\n",
    "        \n",
    "    return dict1\n",
    "    \n",
    "\n",
    "aggreg = sentiments_rdd.reduceByKey(lambda x, y: aggregate_sentiments(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 4.951999999999999,\n",
       "   'neu': 1030.175000000003,\n",
       "   'pos': 196.87100000000038,\n",
       "   'compound': 349.67990000000117}),\n",
       " ('#arsd',\n",
       "  {'neg': 35.58,\n",
       "   'neu': 451.28099999999995,\n",
       "   'pos': 50.13499999999999,\n",
       "   'compound': 35.2231}),\n",
       " ('#iowa',\n",
       "  {'neg': 73.184,\n",
       "   'neu': 521.3799999999999,\n",
       "   'pos': 101.45099999999998,\n",
       "   'compound': 55.191799999999986}),\n",
       " ('#biggboss',\n",
       "  {'neg': 61.392999999999994,\n",
       "   'neu': 872.8369999999999,\n",
       "   'pos': 221.77900000000002,\n",
       "   'compound': 289.0983999999999}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 133.68300000000002,\n",
       "   'neu': 1076.398,\n",
       "   'pos': 147.925,\n",
       "   'compound': 23.1815})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "event_sentiments_l = aggreg.collect()\n",
    "t2 = time.time()\n",
    "time2 = t2-t1\n",
    "\n",
    "event_sentiments_l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentiment(x):\n",
    "    dict1    = x\n",
    "    sum_prob = dict1['neg'] + dict1['neu'] + dict1['pos']\n",
    "    \n",
    "    for key in dict1.keys():\n",
    "        dict1[key] /= sum_prob\n",
    "    return dict1\n",
    "\n",
    "\n",
    "aggreg_normalized = aggreg.map(lambda x: (x[0], normalize_sentiment(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "event_sentiments_ln = aggreg_normalized.collect()\n",
    "t2 = time.time()\n",
    "time3 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#treasure',\n",
       "  {'neg': 0.0040194870446218145,\n",
       "   'neu': 0.8361823639324092,\n",
       "   'pos': 0.15979814902296907,\n",
       "   'compound': 0.2838315484278386}),\n",
       " ('#arsd',\n",
       "  {'neg': 0.06625747677822555,\n",
       "   'neu': 0.8403805614939405,\n",
       "   'pos': 0.0933619617278341,\n",
       "   'compound': 0.06559285357805274}),\n",
       " ('#iowa',\n",
       "  {'neg': 0.1051471591847877,\n",
       "   'neu': 0.7490930511555067,\n",
       "   'pos': 0.1457597896597056,\n",
       "   'compound': 0.07929685423446334}),\n",
       " ('#biggboss',\n",
       "  {'neg': 0.05310771801949638,\n",
       "   'neu': 0.7550434295926761,\n",
       "   'pos': 0.19184885238782745,\n",
       "   'compound': 0.2500831740929352}),\n",
       " ('#thebachelor',\n",
       "  {'neg': 0.09844065490137749,\n",
       "   'neu': 0.792631254942909,\n",
       "   'pos': 0.10892809015571361,\n",
       "   'compound': 0.0170702485850578}),\n",
       " ('#asimriaz',\n",
       "  {'neg': 0.06977859288060737,\n",
       "   'neu': 0.7629025185663331,\n",
       "   'pos': 0.16731888855305946,\n",
       "   'compound': 0.15989261982882397}),\n",
       " ('#curecancer',\n",
       "  {'neg': 0.2291470615972054,\n",
       "   'neu': 0.5569524341721126,\n",
       "   'pos': 0.213900504230682,\n",
       "   'compound': -0.09943524859395293}),\n",
       " ('#rashamidesai',\n",
       "  {'neg': 0.06565979381443297,\n",
       "   'neu': 0.7649621993127148,\n",
       "   'pos': 0.16937800687285218,\n",
       "   'compound': 0.17399931271477664})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_sentiments_ln[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments, aggregating and normalizing: 39.36s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments, aggregating and normalizing: {:.4g}s\".format(time1 + time2 + time3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing average sentiment on all tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_rdd  = words_f.select('filtered').rdd.map(lambda x: ' '.join(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sentences = sentences_rdd.collect()\n",
    "t2 = time.time()\n",
    "time4 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "overall_sent = {'neg': 0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "for sentence in sentences:\n",
    "    sentiment = sid.polarity_scores(sentence)    \n",
    "    for key in overall_sent.keys():\n",
    "        overall_sent[key] += sentiment[key]\n",
    "        \n",
    "    \n",
    "for key in overall_sent.keys():\n",
    "        overall_sent[key] /= num_sentences\n",
    "overall_sent\n",
    "t2 = time.time()\n",
    "time5 = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent computing the sentiments not using an rdd: 82.31s\n"
     ]
    }
   ],
   "source": [
    "print(\"time spent computing the sentiments not using an rdd: {:.4g}s\".format(time4 + time5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "We can clearly see that using an rdd and utilizing the map reduce scheme gives us faster performance! Also, the naive algorithm above doesn't even group by hashtags..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = spark.read.format('json').options(header='true', inferSchema='true') \\\n",
    "  .load('./datasets/NoFilterEnglish2020-02-05.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'display_text_range',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'extended_tweet',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'filter_level',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quote_count',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'quoted_status_permalink',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'timestamp_ms',\n",
       " 'truncated',\n",
       " 'user',\n",
       " 'withheld_in_countries']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+\n",
      "|contributors|coordinates|created_at|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|quoted_status_permalink|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|\n",
      "+------------+-----------+----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+\n",
      "+------------+-----------+----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.filter('retweeted or retweet_count >0 ').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ 'favorite_count', 'favorited', 'id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'quote_count', 'timestamp_ms', 'retweeted']#'retweet_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "## Select Data:\n",
    "features_and_labels = data2.select('favorite_count', 'favorited', 'id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'quote_count', data2.timestamp_ms.cast('float').alias('timestamp_ms'), data2.retweeted.cast('int').alias('retweeted'))\\\n",
    "                           .na.drop(subset = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------------+---------------------+-------------------+-----------+-------------+---------+\n",
      "|favorite_count|favorited|                 id|in_reply_to_status_id|in_reply_to_user_id|quote_count| timestamp_ms|retweeted|\n",
      "+--------------+---------+-------------------+---------------------+-------------------+-----------+-------------+---------+\n",
      "|             0|    false|1224829969655828481|  1224829940929003520|1036435413504798721|          0|1.58085716E12|        0|\n",
      "|             0|    false|1224829969651683328|  1224777400610889728| 786566181071257600|          0|1.58085716E12|        0|\n",
      "|             0|    false|1224829969643106304|  1222855216187478016|           86271989|          0|1.58085716E12|        0|\n",
      "|             0|    false|1224829973833355269|  1224739746947031041|1073232634480738304|          0|1.58085716E12|        0|\n",
      "+--------------+---------+-------------------+---------------------+-------------------+-----------+-------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_and_labels.filter('retweeted ==0').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler().setInputCols(features).setOutputCol('features')\n",
    "features_and_labels = assembler.transform(features_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|(8,[2,3,4,6],[1.2...|\n",
      "|       0.0|         0.0|(8,[2,3,4,6],[1.2...|\n",
      "|       0.0|         0.0|(8,[2,3,4,6],[1.2...|\n",
      "|       0.0|         0.0|(8,[2,3,4,6],[1.2...|\n",
      "|       0.0|         0.0|(8,[2,3,4,6],[1.2...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0\n",
      "GBTClassificationModel: uid = GBTClassifier_af2d8bcef6af, numTrees=10, numClasses=2, numFeatures=8\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"retweeted\", outputCol=\"indexedLabel\").fit(features_and_labels)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol='features', outputCol=\"indexedFeatures\", maxCategories=4).fit(features_and_labels)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = features_and_labels.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
